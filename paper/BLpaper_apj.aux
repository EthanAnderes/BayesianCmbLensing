\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{das2011detection,van2012measurement,planck2013lensing,Polarbear2014,planck2015lensing}
\citation{hu2001mapping,hu2002mass}
\citation{HirataSeljak1,HirataSeljak2}
\citation{Lewis20061}
\newlabel{FirstPage}{{}{1}{}{Doc-Start}{}}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{bezanson2012julia}
\citation{dodelson2003modern}
\@writefile{toc}{\contentsline {section}{\numberline {2}Weak lensing primer and a Bayesian challenge}{2}{section.2}}
\newlabel{primer}{{2}{2}{Weak lensing primer and a Bayesian challenge}{section.2}{}}
\newlabel{data model}{{1}{2}{Weak lensing primer and a Bayesian challenge}{equation.2.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Two parameter analogy}{2}{section.3}}
\newlabel{two parameter system}{{3}{2}{Two parameter analogy}{section.3}{}}
\newlabel{post1}{{2}{2}{Two parameter analogy}{equation.3.2}{}}
\citation{bernardo2003non,gelfand1995efficient,papaspiliopoulos2008stability,papaspiliopoulos2007general,yu2011center}
\citation{elsner2013efficient,jasche2014matrix}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces  {\em  Left:} density contours of the {\bf  ancillary} chain $P( t, \varphi |\text  {data})$ with 20 steps of a Gibbs sampler. {\em  Right:} density contours of the {\bf  sufficient} chain $P(\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle t$}\mathaccent "0365{t}, \varphi |\text  {data})$ with 20 steps of a Gibbs sampler. This illustrates the general heuristic that a slowly converging ancillary chain translates to a quickly converging the sufficient chain.}}{3}{figure.1}}
\newlabel{fastslowGibbs}{{1}{3}{{\em Left:} density contours of the {\bf ancillary} chain $P( t, \varphi |\text {data})$ with 20 steps of a Gibbs sampler. {\em Right:} density contours of the {\bf sufficient} chain $P(\widetilde t, \varphi |\text {data})$ with 20 steps of a Gibbs sampler. This illustrates the general heuristic that a slowly converging ancillary chain translates to a quickly converging the sufficient chain}{figure.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Ancillary versus sufficient parameters for the lensed CMB}{3}{section.4}}
\newlabel{Section: Ancillary and sufficient parameters for the lensed CMB}{{4}{3}{Ancillary versus sufficient parameters for the lensed CMB}{section.4}{}}
\newlabel{suff 1}{{3}{3}{Ancillary versus sufficient parameters for the lensed CMB}{equation.4.3}{}}
\newlabel{suff 2}{{4}{3}{Ancillary versus sufficient parameters for the lensed CMB}{equation.4.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Anti-lensing approximation}{3}{subsection.4.1}}
\newlabel{section: Anti-lensing approximation}{{4.1}{3}{Anti-lensing approximation}{subsection.4.1}{}}
\newlabel{antilense}{{5}{3}{Anti-lensing approximation}{equation.4.5}{}}
\citation{neal2011mcmc}
\citation{PhysRevD.75.083525,taylor2008fast,elsner2010local,2010MNRAS.409..355J,2012MNRAS.425.1042J,jasche2013bayesian,jasche2013methods}
\newlabel{anti approx}{{6}{4}{Anti-lensing approximation}{equation.4.6}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Hamiltonian Monte Carlo sampler for $P(\phi | \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T}, \text  {\rm  data})$}{4}{section.5}}
\newlabel{Section: hamiltonian sampler section}{{5}{4}{Hamiltonian Monte Carlo sampler for $P(\phi | \widetilde T, \text {\rm data})$}{section.5}{}}
\citation{neal2011mcmc}
\citation{taylor2008fast}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces  The difference between anti-lensing and inverse lensing. {\em  Upper left:} anti-lensing potential $-\phi $. {\em  Upper right:} The inverse lensing potential $-\phi ^\text  {inv}$. {\em  Bottom left:} The difference $\phi ^\text  {inv}-\phi $. {\em  Bottom right:} The inverse lensing stream function $-\psi ^\text  {inv}$.}}{5}{figure.2}}
\newlabel{antilensing plots}{{2}{5}{The difference between anti-lensing and inverse lensing. {\em Upper left:} anti-lensing potential $-\phi $. {\em Upper right:} The inverse lensing potential $-\phi ^\text {inv}$. {\em Bottom left:} The difference $\phi ^\text {inv}-\phi $. {\em Bottom right:} The inverse lensing stream function $-\psi ^\text {inv}$}{figure.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {1}{\ignorespaces  $i^\text  {th}$ step of the Hamiltonian Markov Chain}}{5}{algorithm.1}}
\newlabel{ith step of HMC}{{1}{5}{Hamiltonian Monte Carlo sampler for $P(\phi | \widetilde T, \text {\rm data})$}{algorithm.1}{}}
\citation{hu2001mapping,hu2002mass}
\newlabel{grad claim}{{1}{6}{}{claim.1}{}}
\newlabel{grad claim eq}{{7}{6}{}{equation.5.7}{}}
\newlabel{quad estimate}{{11}{6}{Hamiltonian Monte Carlo sampler for $P(\phi | \widetilde T, \text {\rm data})$}{equation.5.11}{}}
\citation{elsner2013efficient}
\citation{jasche2014matrix}
\citation{elsner2013efficient}
\citation{jasche2014matrix}
\citation{wandelt2004cg,eriksen2004cg}
\citation{smith2007mcg}
\citation{seljebotn2014mg}
\citation{elsner2013efficient}
\citation{jasche2014matrix}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces   This graphic illustrates how knowledge of $\phi (x)$, used when sampling from the Gibbs step $P(\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T}|\phi ,\text  {data})$, converts white noise corrupted gridded observations of the lensed CMB into masked observations of the unlensed CMB on a more dense grid. The data associated with the original grid, indexed by $x$, is moved via advection to the lensed grid $x+\nabla \phi (x)$ seen at left. The right panel shows the lensed grid embedded into a higher resolution grid. The data, indexed by the dense regular grid on the right panel, is of the form $T(y)+\mathaccentV {tilde}07En(y)$ where $T(y)$ denotes the unlensed CMB and $\mathaccentV {tilde}07En(y)$ denotes the noise. The unobserved locations, represented by open circles, are characterized with an infinite variance for $\mathaccentV {tilde}07En(y)$. }}{7}{figure.3}}
\newlabel{embed}{{3}{7}{This graphic illustrates how knowledge of $\phi (x)$, used when sampling from the Gibbs step $P(\widetilde T|\phi ,\text {data})$, converts white noise corrupted gridded observations of the lensed CMB into masked observations of the unlensed CMB on a more dense grid. The data associated with the original grid, indexed by $x$, is moved via advection to the lensed grid $x+\nabla \phi (x)$ seen at left. The right panel shows the lensed grid embedded into a higher resolution grid. The data, indexed by the dense regular grid on the right panel, is of the form $T(y)+\tilde n(y)$ where $T(y)$ denotes the unlensed CMB and $\tilde n(y)$ denotes the noise. The unobserved locations, represented by open circles, are characterized with an infinite variance for $\tilde n(y)$}{figure.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Iterative message passing algorithm for $ P(\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T} | \phi ,\text  {\rm  data})$}{7}{section.6}}
\newlabel{Section: iterative message passing section}{{6}{7}{Iterative message passing algorithm for $ P(\widetilde T | \phi ,\text {\rm data})$}{section.6}{}}
\citation{HirataSeljak1,HirataSeljak2}
\@writefile{loa}{\contentsline {algorithm}{\numberline {2}{\ignorespaces Algorithm for sampling from $P(T | \text  {data})$ where $\text  {data}(y) = T(y) + \mathaccentV {tilde}07En(y)$}}{8}{algorithm.2}}
\newlabel{IMP}{{2}{8}{Iterative message passing algorithm for $ P(\widetilde T | \phi ,\text {\rm data})$}{algorithm.2}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Sampling from $P(\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T} | \phi ,\text  {\rm  data})$}}{8}{algorithm.3}}
\newlabel{after HMC}{{3}{8}{Iterative message passing algorithm for $ P(\widetilde T | \phi ,\text {\rm data})$}{algorithm.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Simulation example}{8}{section.7}}
\newlabel{Section: simulation examples}{{7}{8}{Simulation example}{section.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces   This figure shows values of the cooling parameter $\lambda _j$ for $j=1,100, 300, 500, 700, 900$ used in Algorithm \ref  {IMP}. This schedule is applied every $100^\text  {th}$ step in the Gibbs algorithm. In Algorithm $\ref  {IMP}$, the value of $\lambda _j\mathaccentV {bar}016{\sigma }^2dy$ serves as the spectral density of artificial additive white noise in the latent field $M(x)$. Therefore setting $\lambda _j$ greater than $1$, encourages fast mixing of $T_l$ at all frequency vectors $l$ such that $C_l^{TT} \lesssim \lambda _j\mathaccentV {bar}016{\sigma }^2dy$. The cooling schedule shown above is an attempt to let $\lambda _j$ approach $1$ in such a way as to encourage all frequency vectors up to $|l|_\text  {max}$ to mix quickly. }}{9}{figure.4}}
\newlabel{cooling}{{4}{9}{This figure shows values of the cooling parameter $\lambda _j$ for $j=1,100, 300, 500, 700, 900$ used in Algorithm \ref {IMP}. This schedule is applied every $100^\text {th}$ step in the Gibbs algorithm. In Algorithm $\ref {IMP}$, the value of $\lambda _j\bar {\sigma }^2dy$ serves as the spectral density of artificial additive white noise in the latent field $M(x)$. Therefore setting $\lambda _j$ greater than $1$, encourages fast mixing of $T_l$ at all frequency vectors $l$ such that $C_l^{TT} \lesssim \lambda _j\bar {\sigma }^2dy$. The cooling schedule shown above is an attempt to let $\lambda _j$ approach $1$ in such a way as to encourage all frequency vectors up to $|l|_\text {max}$ to mix quickly}{figure.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces  {\em  Left:} simulation truth $\phi (x)$. {\em  Middle:} posterior mean $E(\phi (x)|\text  {data})$. {\em  Right:} The quadratic estimate. To avoid difficulties associated with masked data when using the quadratic estimate, the estimate shown at right is applied to the full data set with the masked region removed. In contrast, the data used for the Bayesian methodology is masked as shown in Figure \ref  {tilde fig}.}}{10}{figure.5}}
\newlabel{phix fig}{{5}{10}{{\em Left:} simulation truth $\phi (x)$. {\em Middle:} posterior mean $E(\phi (x)|\text {data})$. {\em Right:} The quadratic estimate. To avoid difficulties associated with masked data when using the quadratic estimate, the estimate shown at right is applied to the full data set with the masked region removed. In contrast, the data used for the Bayesian methodology is masked as shown in Figure \ref {tilde fig}}{figure.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces  {\em  Upper left:} simulated lensed CMB data with masking and additive white noise (at level $8.0$ $\mu K$ arcmin). {\em  Upper right:} posterior mean $E(T(x)|\text  {data})$. {\em  Lower left:} This plot shows $T(x) - E(T(x)|\text  {data})$ and probes the ability of the Bayesian methodology to delense the observations. This plot should be compared with the nominal difference between the simulation truth unlensed CMB and the lensed CMB, $T(x) - \setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle T$}\mathaccent "0365{T}(x)$, shown {\em  bottom right}. }}{10}{figure.6}}
\newlabel{tilde fig}{{6}{10}{{\em Upper left:} simulated lensed CMB data with masking and additive white noise (at level $8.0$ $\mu K$ arcmin). {\em Upper right:} posterior mean $E(T(x)|\text {data})$. {\em Lower left:} This plot shows $T(x) - E(T(x)|\text {data})$ and probes the ability of the Bayesian methodology to delense the observations. This plot should be compared with the nominal difference between the simulation truth unlensed CMB and the lensed CMB, $T(x) - \widetilde T(x)$, shown {\em bottom right}}{figure.6}{}}
\citation{elsner2013efficient,jasche2014matrix}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces  Here we plot one dimensional slices of the posterior draws from $P(\phi (x)|\text  {data})$ and $P(T(x)|\text  {data})$. For reference, these slices are taken from the horizontal regions at vertical degree mark $12.7^o$ in Figures \ref  {tilde fig} and \ref  {phix fig}. }}{11}{figure.7}}
\newlabel{slice fig}{{7}{11}{Here we plot one dimensional slices of the posterior draws from $P(\phi (x)|\text {data})$ and $P(T(x)|\text {data})$. For reference, these slices are taken from the horizontal regions at vertical degree mark $12.7^o$ in Figures \ref {tilde fig} and \ref {phix fig}}{figure.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces  Estimates of $l^4|\phi _l|^2/4$ and $l^2|T_l|^2$ (shown in blue), scaled to the units of the corresponding spectral density. The red dots show $l^4|\phi _l|^2/4$ and $l^2|T_l|^2$ for the simulation truth (similarly scaled). The discrepancy between the red dots and the spectral densities, shown in black, is exclusively due to cosmic variance. The confidence bars show $95\%$ probability regions from the posterior distributions $P(l^4|\phi _l|^2/4\tmspace  +\thinmuskip {.1667em}|\tmspace  +\thinmuskip {.1667em}\text  {data})$ and $P(l^2|T_l|^2\tmspace  +\thinmuskip {.1667em}|\tmspace  +\thinmuskip {.1667em}\text  {data})$. }}{11}{figure.8}}
\newlabel{p spec fig}{{8}{11}{Estimates of $l^4|\phi _l|^2/4$ and $l^2|T_l|^2$ (shown in blue), scaled to the units of the corresponding spectral density. The red dots show $l^4|\phi _l|^2/4$ and $l^2|T_l|^2$ for the simulation truth (similarly scaled). The discrepancy between the red dots and the spectral densities, shown in black, is exclusively due to cosmic variance. The confidence bars show $95\%$ probability regions from the posterior distributions $P(l^4|\phi _l|^2/4\,|\,\text {data})$ and $P(l^2|T_l|^2\,|\,\text {data})$}{figure.8}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Concluding remarks}{11}{section.8}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces  This plot summaries the correlation, in $\Delta l$ wavenumber bins, between the simulation truth and their corresponding posterior samples. In particular, samples were generated from $\frac  {1}{c}\DOTSB \sum@ \slimits@ _{l\in \Delta l} \phi ^\text  {sim}_l\phi ^*_l $ and $\frac  {1}{c}\DOTSB \sum@ \slimits@ _{l\in \Delta l} T^\text  {sim}_lT^*_l $ where $\Delta l$ is a frequency wavenumber bin, $\phi _l$ and $T_l$ are the simulation truth, $\phi ^\text  {sim}_l$ and $T^\text  {sim}_l$ is sampled from the Gibbs algorithm presented here and $c$ is a normalization constant which transforms to a correlation scale. Recall that $|l|_\text  {max}$ for the lensing potential is $\sim 460$ and is $\sim 2700$ for the unlensed CMB, which explains why the correlation for $\phi _l$ only extends to $460$. Notice that the plotted correlations trend to $0$ for larger wavenumber. This is what one would expect from the Bayesian posterior. Indeed, the data is less informative at larger wavenumber. This causes the Bayesian posterior to revert to the prior, which will be uncorrelated with the simulation truth. }}{12}{figure.9}}
\newlabel{t spec fig}{{9}{12}{This plot summaries the correlation, in $\Delta l$ wavenumber bins, between the simulation truth and their corresponding posterior samples. In particular, samples were generated from $\frac {1}{c}\sum _{l\in \Delta l} \phi ^\text {sim}_l\phi ^*_l $ and $\frac {1}{c}\sum _{l\in \Delta l} T^\text {sim}_lT^*_l $ where $\Delta l$ is a frequency wavenumber bin, $\phi _l$ and $T_l$ are the simulation truth, $\phi ^\text {sim}_l$ and $T^\text {sim}_l$ is sampled from the Gibbs algorithm presented here and $c$ is a normalization constant which transforms to a correlation scale. Recall that $|l|_\text {max}$ for the lensing potential is $\sim 460$ and is $\sim 2700$ for the unlensed CMB, which explains why the correlation for $\phi _l$ only extends to $460$. Notice that the plotted correlations trend to $0$ for larger wavenumber. This is what one would expect from the Bayesian posterior. Indeed, the data is less informative at larger wavenumber. This causes the Bayesian posterior to revert to the prior, which will be uncorrelated with the simulation truth}{figure.9}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces  This plot illustrates the Gibbs chain correlation length scale and the speed of mixing for different statistics of the lensing potential. The left plot shows the Gibbs chain for $\phi (x)$ where $x = (9.9^o, 13.2^o)$ and $x = (1.6^o, 1.6^o)$ in the same degree coordinates given in Figures \ref  {tilde fig} and \ref  {phix fig}. The right plot shows the real and imaginary parts of $\phi _l$ where the frequency vector $l$ is set to $(126.56,63.28)$. Each dashed line represents the corresponding simulation truth parameters. Recall that the algorithm is initialized with a zero lensing potential. }}{12}{figure.10}}
\newlabel{assessing convergence fig}{{10}{12}{This plot illustrates the Gibbs chain correlation length scale and the speed of mixing for different statistics of the lensing potential. The left plot shows the Gibbs chain for $\phi (x)$ where $x = (9.9^o, 13.2^o)$ and $x = (1.6^o, 1.6^o)$ in the same degree coordinates given in Figures \ref {tilde fig} and \ref {phix fig}. The right plot shows the real and imaginary parts of $\phi _l$ where the frequency vector $l$ is set to $(126.56,63.28)$. Each dashed line represents the corresponding simulation truth parameters. Recall that the algorithm is initialized with a zero lensing potential}{figure.10}{}}
\bibdata{refs}
\bibcite{das2011detection}{{1}{2011}{{Das et~al.}}{{}}}
\bibcite{van2012measurement}{{2}{2012}{{Engelen et~al.}}{{}}}
\bibcite{planck2013lensing}{{3}{2014}{{{Planck Collaboration}}}{{}}}
\bibcite{Polarbear2014}{{4}{2014}{{{The Polarbear Collaboration: P.~A.~R.~Ade} et~al.}}{{{The Polarbear Collaboration: P.~A.~R.~Ade}, {Akiba}, {Anthony}, {Arnold}, {Atlas}, {Barron}, {Boettger}, {Borrill}, {Chapman}, {Chinone} et~al.}}}
\bibcite{planck2015lensing}{{5}{2015}{{{Planck Collaboration}}}{{}}}
\bibcite{hu2001mapping}{{6}{2001}{{Hu}}{{}}}
\bibcite{hu2002mass}{{7}{2002}{{Hu and Okamoto}}{{}}}
\bibcite{HirataSeljak1}{{8}{2003{a}}{{Hirata and Seljak}}{{}}}
\bibcite{HirataSeljak2}{{9}{2003{b}}{{Hirata and Seljak}}{{}}}
\bibcite{Lewis20061}{{10}{2006}{{Lewis and Challinor}}{{}}}
\bibcite{bezanson2012julia}{{11}{2012}{{Bezanson et~al.}}{{Bezanson, Karpinski, Shah, and Edelman}}}
\bibcite{dodelson2003modern}{{12}{2003}{{Dodelson}}{{}}}
\bibcite{bernardo2003non}{{13}{2003}{{Roberts et~al.}}{{Roberts, Papaspiliopoulos, and Sk\"old}}}
\bibcite{gelfand1995efficient}{{14}{1995}{{Gelfand et~al.}}{{Gelfand, Sahu, and Carlin}}}
\bibcite{papaspiliopoulos2008stability}{{15}{2008}{{Papaspiliopoulos and Roberts}}{{}}}
\bibcite{papaspiliopoulos2007general}{{16}{2007}{{Papaspiliopoulos et~al.}}{{Papaspiliopoulos, Roberts, and Sk{\"o}ld}}}
\bibcite{yu2011center}{{17}{2011}{{Yu and Meng}}{{}}}
\bibcite{elsner2013efficient}{{18}{2013}{{{Elsner} and {Wandelt}}}{{}}}
\bibcite{jasche2014matrix}{{19}{2015}{{{Jasche} and {Lavaux}}}{{}}}
\bibcite{neal2011mcmc}{{20}{2011}{{Neal}}{{}}}
\bibcite{PhysRevD.75.083525}{{21}{2007}{{Hajian}}{{}}}
\bibcite{taylor2008fast}{{22}{2008}{{Taylor et~al.}}{{Taylor, Ashdown, and Hobson}}}
\bibcite{elsner2010local}{{23}{2010}{{Elsner and Wandelt}}{{}}}
\bibcite{2010MNRAS.409..355J}{{24}{2010}{{Jasche et~al.}}{{Jasche, Kitaura, Li, and En{\ss }lin}}}
\bibcite{2012MNRAS.425.1042J}{{25}{2012}{{Jasche and Wandelt}}{{}}}
\bibcite{jasche2013bayesian}{{26}{2013{a}}{{Jasche and Wandelt}}{{}}}
\bibcite{jasche2013methods}{{27}{2013{b}}{{Jasche and Wandelt}}{{}}}
\bibcite{wandelt2004cg}{{28}{2004}{{{Wandelt} et~al.}}{{{Wandelt}, {Larson}, and {Lakshminarayanan}}}}
\bibcite{eriksen2004cg}{{29}{2004}{{{Eriksen} et~al.}}{{{Eriksen}, {O'Dwyer}, {Jewell}, {Wandelt}, {Larson}, {G{\'o}rski}, {Levin}, {Banday}, and {Lilje}}}}
\bibcite{smith2007mcg}{{30}{2007}{{{Smith} et~al.}}{{{Smith}, {Zahn}, and {Dor{\'e}}}}}
\bibcite{seljebotn2014mg}{{31}{2014}{{{Seljebotn} et~al.}}{{{Seljebotn}, {Mardal}, {Jewell}, {Eriksen}, and {Bull}}}}
\newlabel{dr1}{{\hbox {1}}{13}{Acknowledgments}{equation.Alph0.1}{}}
\newlabel{dr2}{{\hbox {2}}{13}{Acknowledgments}{equation.Alph0.2}{}}
\newlabel{grad of prior}{{\hbox {3}}{13}{Acknowledgments}{equation.Alph0.3}{}}
\newlabel{ll1}{{\hbox {4}}{14}{Acknowledgments}{equation.Alph0.4}{}}
\newlabel{ll2}{{\hbox {5}}{14}{Acknowledgments}{equation.Alph0.5}{}}
\newlabel{partialconj}{{1}{14}{}{lemma.1}{}}
\newlabel{partial1}{{\hbox {10}}{14}{Acknowledgments}{equation.Alph0.10}{}}
\newlabel{partial2}{{\hbox {11}}{14}{Acknowledgments}{equation.Alph0.11}{}}
\bibstyle{apsrev}
\newlabel{forreal}{{2}{15}{}{lemma.2}{}}
\newlabel{conv}{{3}{15}{}{lemma.3}{}}
\newlabel{LastPage}{{}{16}{}{page.16}{}}
