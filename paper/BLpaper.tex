\documentclass[noinfoline]{imsart}


\usepackage{bm}
\usepackage{graphics,epsfig,rotate,lscape,graphicx,amsmath,amsthm,amssymb,float,amsfonts,amsbsy,hyperref,delarray,sectsty,amsfonts,amscd,pifont}
\usepackage{color,multirow}
\usepackage{algorithmicx}
\usepackage{algorithm}
\usepackage{enumerate}
\usepackage{microtype}

\usepackage{geometry}
\geometry{letterpaper,left=1.2in,right=1.2in,top=1.2in,bottom=1.1in}

\bibliographystyle{plain}


\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{notation}{Notation}
\renewcommand{\labelenumi}{(\roman{enumi})}
\newcommand{\bx}{\boldsymbol x}
\newcommand{\by}{\boldsymbol y}
\newcommand{\bl}{\boldsymbol l}
\newcommand{\bk}{\boldsymbol k}
\newcommand{\re}{\text{\,\rm re}}
\newcommand{\im}{\text{\,\rm im}}
\newcommand{\bs}{\boldsymbol}



% --------------------------------------
\begin{document}



\begin{frontmatter}
\title{Bayesian estimates of CMB gravitational lensing}
\runtitle{Bayesian CMB lensing}
\begin{aug}
   \author{\fnms{Ethan}  \snm{Anderes}\corref{}\thanksref{t1}\thanksref{a}}
   \author{\fnms{Benjamin} \snm{Wandelt}\thanksref{b}}
   \and
   \author{\fnms{Guilhem}  \snm{Lavaux}\thanksref{b}}%
   \ead[label=u1, url]{https://github.com/EthanAnderes/BayesianCmbLensing.git}
   \address[a]{Department of Statistics, University of California, Davis CA 95616, USA.}
   \address[b]{Institut d' Astrophysique de Paris. }
   \thankstext{t1}{Research supported by: NSF DMS-1007480, NSF CAREER DMS-1252795}
   \thankstext{t2}{Code available at \printead{u1}}
   \runauthor{E. Anderes et al.}
\end{aug}

\begin{abstract} 
The Plank satellite along with
ground based telescopes such as the Atacama Cosmology Telescope (ACT)  and  the South Pole Telescope (SPT) have mapped the  cosmic microwave background (CMB) at such an unprecedented resolution as too allow a detection of the subtle distortions  due to the gravitational influence of intervening  dark matter. This distortion is called  gravitational lensing and has become a  powerful probe of cosmology and the nature of dark matter.
Estimating the gravitational lensing of the CMB  is important for two reasons. First, the weak lensing estimates can be used to construct a map of dark matter which would be invisible otherwise.
Second,  weak lensing estimates can, in principle, un-lense the observed CMB to construct the original CMB radiation fluctuations. Both of these maps,  the un-lensed CMB radiation field and the dark matter field, are deep probes into the nature of cosmology and cosmic structure. Bayesian techniques seem a perfect fit for the statistical analysis of lensing and the CMB. One reason is that both the unlensed CMB and the lensing potential are---very nearly---realizations of Gaussian random fields. The Gaussianity coming from physically predicted   quantum randomness in the early universe. However, challenges associated with a full Bayesian analysis has prevented previous attempts at  developing a working Bayesian prototype. 
In this paper we solve many of these obstacles with a re-parameterization of the na\"ive lensing problem. This  allows us to obtain approximate draws from the  Bayesian lensing posterior of both the lensing potential and the un-lensed CMB which converges remarkably fast. 
\end{abstract}

\begin{keyword}
\kwd{CMB}
\kwd{gravitational lensing}
\kwd{Bayesian}
\kwd{Gibbs sampler}
\end{keyword}

\end{frontmatter}





 Over the past few years, data from ground based telescopes (ACT, SPT and Bicep2) and the Plank satellite and have resulted in an unprecedented detection of weak gravitational lensing of the cosmic microwave background (CMB) \textcolor{red}{[add citations]}.  This new data not only probes the nature of dark matter but also constrains cosmological models of gravity waves and dark energy \textcolor{red}{[add citations]}. The state-of-the-art estimator of CMB gravitational lensing, the quadratic estimator developed by Hu and Okomoto (2001, 2002), works in part through a delicate cancellation of terms in an infinite Taylor expansion of the lensing effect on the CMB. The effect of this cancellation is particularly sensitive to foreground contaminants and sky masking, which  if not fully accounted for,  limits  the statistical inferential power of this new data.  

 Possibly the most promising alternative to the quadratic estimator is Bayesian lensing. Indeed, Bayesian techniques applied to the lensed CMB observations have the potential for drastically changing the way lensing is estimated and used for inference.  Current frequentest estimators of the unknown lensing potential treat the unlensed CMB as a source of shape noise which is marginalized out. Conversely, a Bayesian lensing posterior treats the lensing potential {\em and} the unlensed CMB as joint unknowns, whereby obtaining scientific constrains jointly rather than marginally. Moreover, the  posterior distribution is easier to interpret and sequentially update with additional data. From the geometry of weak lensing, most of the lensing power comes from matter at a redshift $z\approx 2$ \textcolor{red}{(check the facts here)}. At these distances the matter distribution is well approximated by linear theory which predicts the matter density fluctuations are nearly Gaussian. In addition, the unlensed CMB is, at present, indistinguishable from an isotropic Gaussian random field.  From a statistical perspective, this is a perfect scenario for Bayesian methods in that both the observations and the unknown lensing potential are {\em physically predicted} to be Gaussian random fields.  
 
Physicists have known, for some time, that Bayesian methods could potentially provide next-generation lensing estimates. In their seminal review \cite{Lewis20061}, authors Lewis and Challinor  discuss the possibility of obtaining posterior draws from the lensing potential and the unlensed CMB jointly. However, they acknowledge the main obstacle for naive Gibbs implementations:
\begin{quote}
``... given a particular lensing potential the delensed sky is given essentially by a delta function.
This means that naive Gibbs iterations will not converge within a reasonable time. At the time of writing there are no known practical methods for sampling from the full posterior distribution."
\end{quote}
In this paper we show that, indeed, there does exist a practical way to obtain Gibbs iterations which converge quickly. The solution is through a re-parameterization of CMB lensing problem. Instead of treating the lensing potential as unknown we work with inverse-lensing or what we call anti-lensing. Surprisingly, the slowness of naive Gibbs translates to fast convergence of the re-parameterized Gibbs chain. 



In Section \ref{two parameter system} we motivate our re-parameterization by analyzing a simple two parameter statistical problem.  The concepts are then applied to the Bayesian lensing problem in Section \ref{Section: Ancillary and sufficient parameters for the lensed CMB}. The two conditional distributions in our Gibbs implementation are discussed in Section \ref{Section: hamiltonian sampler section} and Section \ref{Section: iterative message passing section}. We finish with some  simulation examples in Section \ref{Section: simulation examples}.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Weak lensing primer and a Bayesian challenge}
\label{primer}

In this section we describe the  basics of CMB lensing and Bayesian estimation.  The effect of weak lensing is to simply remap the CMB, preserving surface brightness.   Up to leading order, the remapping  displacements are given by $\nabla \phi$, where $\phi$ denotes a lensing potential and is the planer projection of a three dimensional gravitational potential (see Dodelson, S. (2003), for example). Therefore the lensed CMB can  be written $T(\bx + \nabla \phi(x))$ where $T(x)$ denotes the unlensed CMB temperature fluctuations projected to the observable sky wand $x$ represents an observational direction on the unit sphere. For this paper we will be focusing on the small angle limit  so that $x$ is assumed to vary in a small patch of $ \Bbb R^2$. The lensed CMB is observed with additive noise (denoted $n(x)$) due to instrumental shot noise and a beam effect so  that the data can be written in the following form
\begin{equation}
\label{data model}
 \text{data}(x)= T(x + \nabla \phi(x))+ n(x).
\end{equation}
The goal of weak lensing surveys is to use the data in (\ref{data model}) to  estimate $\phi$ and $T$ or the spectral densities of $T$ and $\phi$.  

A very natural approach to develop a Bayesian lensing estimator is to generate posterior samples through a Gibbs algorithm which iteratively samples from the two conditionals: $P(T |  \phi,\text{data})$ and $P(\phi | T,  \text{data})$.
Sampling from $P(T |  \phi,\text{data})$ is simply a Gaussian random field prediction problem since conditioning on $\phi$ models the data as
\[ \text{data}(x) = T(\!\!\!\underbrace{x+\nabla\phi(x)}_\text{\tiny known obs locations}\!\!\!) + n(x).\]
In other words, he data is a noisy version of  $T$ observed on an irregular grid. 
Conversely, when sampling from $P(\phi | T,\text{data})$ the data is of the form
\[ \text{data}(x) = \underbrace{\!\!T\!\!}_{\text{\tiny known}}(x+\nabla\phi(x)) + n(x). \]
To see how one might approximate this conditional notice first that the CMB field $T(x)$ is very smooth. Indeed, silk dampining \textcolor{red}{[cite]} predicts  an expontially decaying power spectrum. Therefore a linear Taylor approximation,  $\text{data(x)} \approx T(x) + \nabla T(x)\cdot \nabla\phi(x) + n(x)$, may be useful. In fact, the quadratic estimator explicitly uses this linear approximation in the construction of it's estimate. 
If one is willing to use this linear approximation then the conditional $P(\phi | T,\text{data})$ is simply a Bayesian regression problem since $T$ (and thus $\nabla T$) are both known with a Gaussian prior on $\nabla \phi$.

Unfortunately, the structure of both of these conditionals make the Gibbs very slow to converge. The case is exacerbated in the situation when noise level is small. For example, in the second conditional, if $T$ is known and fixed, the extent of the  $\phi$'s which are possible under $P(\phi|T,\text{data})$  is very small compared to the possible $\phi$'s  in $P(\phi, T| \text{data})$ when $T$ is allowed to vary. 
This suggests a highly dependent posterior $P(\phi, T| \text{data})$. 
This was also noticed by \textcolor{red}{[Cite Lewis and Challanore]} for the first conditional.









%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two parameter analogy}
\label{two parameter system}


To motivate our solution to the Bayesian lensing problem we start with a simple two parameter statistical problem.  This system has two unknown parameters $ t, \varphi$ with data given by
\[\text{data} =  t + \varphi + n\]
where $n$ denotes additive noise.  In the Bayesian setting, the posterior distribution is computed as 
\begin{equation}
\label{post1}
 P( t,\varphi|\text{data})\propto P(\text{data}| t, \varphi) P( t,\varphi) 
 \end{equation}
where $P(\text{data}| t, \varphi)$ denotes the likelihood of the data given  $ t, \varphi$ and $P( t,\varphi)$ denotes the  prior on $ t, \varphi$. 
The Gibbs sampler is a widely used algorithm for generating (asymptotic) samples from  $P( t, \varphi|\text{data})$ \textcolor{red}{[add citations]}. The algorithm generates a Markov chain of parameter values $( t^{1}, \varphi^{1}), ( t^{2}, \varphi^{2}),\ldots$ generated by iteratively sampling from the conditional distributions:
\begin{align*}
 t^{i+1} &\sim P( t|\varphi^{i},\text{data}) \\
\varphi^{i+1}   &\sim P(\varphi| t^{i+1},\text{data}).
\end{align*}
A useful heuristic for determining the convergence rate of a Gibbs chain is the extent to which the two parameters $ t$ and $\varphi$ are dependent in $P(\varphi,  t|\text{data})$. A highly dependent posterior $P( t, \varphi|\text{data})$ leads to a slow Gibbs chain, near independence leads to a fast Gibbs chain. Indeed, exact independence gives a sample of the posterior after one Gibbs step.  A technique for accelerating the convergence of a Gibbs sampler is to find a  re-parameterization of $ t$ and $\varphi$ in a way which makes the posterior less dependent. In the remainder of this section we discuss a specific re-parameterization which, by analogy, can be applied to Bayesian lensing.


The relevant situation for Bayesian lensing is the case that $ t$ and $\varphi$ are highly negatively correlated in $P( t, \varphi|\text{data})$.  This motivates re-parameterizing $( t,\varphi)$ to $(\widetilde  t, \varphi)$ where $\widetilde  t \equiv  t + \varphi$ so that
\begin{align*}
\text{data} &= \widetilde  t + n.
\end{align*}
In the statistics literature,  $( t, \varphi)$ is commonly referred to as an {\bf ancillary parameterization} whereas $(\widetilde  t, \varphi)$ is referred to as a {\bf sufficient parameterization} \textcolor{red}{[add citations]}. Figure  \ref{fastslowGibbs} illustrates the difference between an ancillary versus sufficient posterior distribution for our simple two parameter model. The left plot shows the posterior density contours for the ancillary parameterization $( t, \varphi)$, along with 40 steps of a Gibbs sampler.  Conversely, the right plot shows the posterior density contours for the sufficient chain $(\widetilde  t, \varphi)$ with 40 Gibbs steps. Notice that negative correlation  in the ancillary parameterization manifests in near independence for the sufficient chain.  Indeed, the slower the ancillary chain the faster the sufficient chain and vice-versa. 
\begin{figure}[H]
\label{fastslowGibbs}
\includegraphics[height=2.0in]{graphics/theta.pdf}\includegraphics[height=2.0in]{graphics/thetatilde.pdf}
\caption{{\em Left:} density contours of the {\bf ancillary} chain $P( t, \varphi|\text{data})$ with 40 steps of a Gibbs sampler. {\em Right:} density contours of the {\bf sufficient}  chain $P(\widetilde  t, \varphi|\text{data})$ with 40 steps of a Gibbs sampler.}
\end{figure}
 







	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ancillary versus sufficient parameters for the lensed CMB}
\label{Section: Ancillary and sufficient parameters for the lensed CMB}

The ancillary parameterization presented in the previous section is analogous to the lensed CMB problem as follows
\begin{align*}
  \text{data}(x) &= T(x+\nabla \phi(x)) + n(x)\quad\text{\bf\em analogous to}\quad
  \text{data} =  t +\varphi + n
\end{align*}
where the unlensed  CMB temperature field $T$ and the lensing potential $\phi$ are the two unknown parameters. As was discussed in Section \ref{primer} the Gibbs chain based on the ancillary parameters $T(x)$ and $\phi(x)$ is exceedingly slow.  This clearly motivates the following re-parameterization to sufficient parameters for the lensed CMB problem 
\begin{align*}
  \text{data}(x) &= \widetilde T(x) + n(x) \quad\text{\bf\em analogous to}\quad
  \text{data} =  \widetilde t + n
\end{align*}
where now $\widetilde T$ denotes the lensed CMB temperature field with no noise or beam.
The sufficient chain then proceeds as
\begin{align}
\label{suff 1} \widetilde T^{i+1}&\sim P(\widetilde T |  \phi^{i},\text{data}) \\
\label{suff 2} \phi^{i+1}&\sim P(\phi | \widetilde T^{i+1},  \text{data}).
\end{align}
In Section \ref{Section: hamiltonian sampler section} we derive a Hamiltonian Markov Chain algorithm to sample from (\ref{suff 2}). In Section \ref{Section: iterative message passing section} we adapt an iterative message passing algorithm, originally developed in \textcolor{red}{[add citation]}, for sampling from (\ref{suff 1}). Both these algorithms rely heavily on an approximation---motivated again by the two parameter system---we call {\em anti-lensing}.







%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Anti-lensing approximation}

In the two parameter analogy from Section \ref{two parameter system},  the relation between the sufficient parameter $\widetilde t$ and the ancillary parameter $t$ is given by $\widetilde t - \varphi = t$. The corresponding relation for CMB lensing we refer to as {\em anti-lensing}
\begin{equation}
\label{antilense}
 \widetilde T(\underbrace{x-\nabla\phi(x)}_\text{\small anti-lensing}) \approx T(x).% \quad\text{\bf\em analogous to}\quad \widetilde t - \varphi = t.
\end{equation}
Indeed,  anti-lensing is a key concept in this paper and allows sampling from both distributions (\ref{suff 1}) and (\ref{suff 2}).
We distinguish between {\em inverse lensing} and  {\em anti-lensing}. Inverse lensing denotes the true coordinate displacement which, when applied to  $\widetilde T$, recovers the unlensed $T$. Conversely  anti-lensing is  given by $-\nabla\phi$ and approximates inverse lensing.  
To examine the difference between these two operations, start with a Helmholtz decomposition of the inverse lensing displacement: $-\nabla \phi^\text{inv}(x) - \nabla^\perp \psi^\text{inv}(x),$  where $\nabla^\perp \equiv \bigr(-\frac{\partial}{\partial y},\frac{\partial}{\partial x} \bigl)$ and  $\psi^\text{inv}$ denotes a stream function potential which models a field rotation. Now $\phi^\text{inv}$ and $\psi^\text{inv}$ are used to convert a lensed CMB $\widetilde T$ to $T$ as follows 
\begin{align*}
\widetilde T\bigl(x-\nabla \phi^\text{inv}(x) - \nabla^\perp \psi^\text{inv}(x)\bigr) &= T(x).
\end{align*}
Due to the fact that the expected size of the lensing displacement $\nabla\phi$ is much smaller than the correlation length scale of $\phi$ we have
\begin{align}
\label{anti approx}
-\nabla\phi &\approx -\nabla\phi^\text{inv} \approx -\nabla\phi^\text{inv}- \nabla^\perp \psi^\text{inv}.
\end{align}
Indeed, the typical displacement size $\nabla \phi(x)$ is less than 3 arcmin whereas the correlation length scale of $\phi$ is on the order of degrees. In Figure \ref{antilensing plots} we show a simulation of  $-\phi$ (upper left) with the corresponding inverse lensing potential $-\phi^\text{inv}$ (upper right). The difference $\phi - \phi^\text{inv}$ is also shown (bottom left) along with the stream function $-\psi^\text{inv}$. Clearly, the magnitude of the difference  $\phi^\text{inv}-\phi$ and $-\psi^\text{inv}$ is sub-dominant to estimation error expected in current lensing experimental conditions.


\begin{figure}[t]
\label{antilensing plots}
\includegraphics[height=2.4in]{graphics/anti_plt1.pdf}\includegraphics[height=2.4in]{graphics/anti_plt2.pdf}\\%
\includegraphics[height=2.4in]{graphics/anti_plt3.pdf}\includegraphics[height=2.4in]{graphics/anti_plt4.pdf}
\caption{The difference between anti-lensing and inverse lensing. {\em Upper left:} anti-lensing potential $-\phi$. {\em Upper right:} The inverse lensing potential $-\phi^\text{inv}$. {\em Bottom left:} The difference $\phi^\text{inv}-\phi$. {\em Bottom right:} The inverse lensing stream function $-\psi^\text{inv}$.}
\end{figure}


Regardless of the fact that $-\nabla\phi$ is a good approxiamtion to inverse lensing, the Hamiltonian sampler for $P(\phi | \widetilde T,  \text{\rm data})$, described below in Section \ref{Section: hamiltonian sampler section},  can be easily adjusted to sample from the inverse lensing stream and potential functions jointly $P(\phi^\text{inv},\psi^\text{inv} | \widetilde T,  \text{\rm data})$.   However, we have excluded it from our analysis since the magnitude of the lensing potential is at least an order of magnitude smaller than estimation error. 










%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Hamiltonian Monte Carlo sampler for $P(\phi | \widetilde T,  \text{\rm data})$}
\label{Section: hamiltonian sampler section}


Throughout the remainder of this paper the Fourier transform of any function $f(x)$ will be denoted by $f_l$ or $f_k$ so that
$f_l  =  \int_{\Bbb R^2} e^{-i x\cdot l}  f(x)\frac{dx}{2\pi}$ and
$f(x) =  \int_{\Bbb R^2} e^{i x\cdot l}  f_l \frac{dl}{2\pi}$ 
where $l\in \Bbb R^2$ is a two dimensional frequency vector and $x\in \Bbb R^2$  is a two dimensional spatial coordinate.


The follow claim shows that the gradient of the log density of $P(\phi|\widetilde T, \text{data})$, with respect to the Fourier basis of $\phi$, can be computed quickly with Fourier and inverse Fourier transforms. This claim makes the Hamiltonian Monte Carlo (HMC) algorithm possible.

\begin{claim}
\label{grad claim}
 Under the anti-lensing approximation (\ref{antilense}) for any nonzero frequecy vector $l\equiv (l_1, l_2) \in \Bbb R^2$ 
 \[\frac{\partial}{\partial \phi_l}\log P(\phi | \widetilde T,  \text{\rm data}) \propto -  \frac{\phi_l}{C^{\phi\phi}_{l}} -  \sum_{q=1,2} i  l_q \int_{\Bbb R^2} e^{-i x\cdot l} A^q(x)B(x) \frac{dx}{2\pi}   \]
 where  $\phi_l = \re \phi_l + i \im \phi_l$, $\frac{\partial}{\partial \phi_l}\equiv \frac{\partial}{\partial\re \phi_l} + i \frac{\partial}{\partial\im\phi_l}$ and 
 \begin{align}
 B_l &\equiv \frac{1}{C_l^{TT}} \int e^{-i x\cdot l}  \widetilde T(x-\nabla \phi(x))\frac{dx}{2\pi} \\ 
 A^q(x) &\equiv \frac{\partial\widetilde T}{\partial x_q}\bigl(x-\nabla \phi(x)\bigr).
 \end{align}
\end{claim}

{\rm Remark:}
We also remark that the gradient is also an un-normalized quadratic destimate when the noise is zero and ...



The Hamiltonian Monte Carlo (HMC) algorithm is an iterative sampling algorithm designed to mitigate the low-acceptance rate of the Metropolis-Hastings algorithm when working in high dimension. A nice review of HMC can be found in \textcolor{red}{[cite Neal's paper]}. For applications of HMC in cosmology see \textcolor{red}{[add citations]}. We give a brief review here. 

Let $\bs \phi$ denote the concatenation of the real and imaginary parts of $\phi_l$ as $l$ ranges through discrete frequencies $l$ ranging up to a pre-specified $|l|_{max}$ (but excluding  half of the Fourier frequencies due to the Hermitian symmetries associated with the Fourier transform of a real field). Note that $\bs \phi$ is a vector of real numbers. 
Let $P(\bs \phi|\widetilde T, \text{data})$ denote the density of $\bs \phi$ given $\widetilde T$ and the data. 
Let $\bs p$ denote a `momentum' vector and $\bs m$ denote a the `mass' vector, which are both the same length as $\bs \phi$. The Hamiltonian is a function of $\bs \phi$ and $\bs p$ and is defined as follows
\[ H(\bs \phi, \bs p):= -\log P(\bs \phi|\widetilde T, \text{data})+\sum_k \frac{\bs p_k^2}{2\bs m_k^2}. \]
This Hamiltonian generates a time-dependent evolution of $\bs \phi$ and $\bs p$ given by 
\begin{align*}
\frac{d\bs \phi^t}{dt} &= \phantom{-}\nabla_{\bs p} H(\bs \phi^t, \bs p^t) \\
\frac{d\bs p^t}{dt}    &= -\nabla_{\bs \phi} H(\bs \phi^t, \bs p^t).
\end{align*} 
The HMC is a discrete version of  this time-dynamic equation, using a leapfrog method, which produces a Markov chain $(\bs \phi_1, \bs p_1), (\bs \phi_2, \bs p_2), \ldots$ where the $i+1$ iteration is given by the following algorithm
\begin{algorithm}[H]
\small
\caption{ $i^\text{th}$ step of the Hamiltonian Markov Chain}
\label{ith step of HMC}
\begin{algorithmic}[1]
\State Set $\bs \phi^0:= \bs \phi_{i-1}$ and simulate $\bs p^0 \sim \mathcal N(0,\Lambda_m^2 )$ where $\Lambda_m$ is diagonal with  $diag(\Lambda_m)=\bs m$.
\State  Recursively compute $\bs \phi^{k\epsilon}$ and $\bs p^{k\epsilon}$ for $k=1,\ldots, n$ using the following equations:
\begin{align*}
\bs \phi^{t+\epsilon} &:= \bs \phi^{t} + \epsilon \Lambda_m^{-1} \left[ \bs p^t - \frac{\epsilon}{2} \nabla_{\bs \phi} H(\bs \phi^t, \bs p^t) \right]. \\
\bs p^{t+\epsilon} &:= \bs p^{t} - \frac{\epsilon}{2}\Bigl[ \nabla_{\bs \phi} H(\bs \phi^t, \bs p^t) + \nabla_{\bs \phi} H(\bs \phi^{t+\epsilon}, \bs p^t)\Bigr]
\end{align*}
% \begin{align*}
% \bs p^{t+\epsilon/2} &:= \bs p^{t} + \frac{\epsilon}{2} \nabla_{\bs \phi} H(\bs \phi^t, \bs p^t) \\
% \bs \phi^{t+\epsilon} &:= \bs \phi^{t} + \frac{\epsilon}{2} \nabla_{\bs \phi} H(\bs \phi^t, \bs p^t) \\
% \bs p^{(k+2)\epsilon} &:= \bs p^{(k+2)\epsilon/2} - \frac{\epsilon}{2}\Bigl[ \nabla_{\bs \phi} H(\bs \phi^t, \bs p^t) + \nabla_{\bs \phi} H(\bs \phi^{t+\epsilon}, \bs p^t)\Bigr]
% \end{align*}
\State Simulate $u\sim \mathcal U(0,1)$, and define $p:= \min\left(1,{e^{- H(\bs \phi^{n\epsilon}, \bs p^{n\epsilon})}}/{e^{-H(\bs \phi^0, \bs p^0)}}\right)$.
\State  If $u< p$,  set $\bs \phi_{i}:= \bs \phi^{n\epsilon}$, otherwise set  $\bs \phi_{i}:= \bs \phi_{i-1}$.
\end{algorithmic}
\end{algorithm}

The HMC algorithm is notoriously sensitive to tuning parameters. 
The prevailing wisdom is that one should set $\bs m$ to match the posterior variance. For the problems given in the simulation simply set  $\bs m$ to be nearly proportional  to $C_l^{\phi\phi}$ (with slightly more attenuation at low wavenumber).  As is advocated in \textcolor{red}{[cite it]} we  use a random $\epsilon$ to avoid resonant frequencies.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Iterative message passing algorithm for $ P(\widetilde T |  \phi,\text{\rm data})$}
\label{Section: iterative message passing section}



Start by  transforming each pixel location $x$ to the pre-lense location $x+\nabla \phi(x)$, while simultaneously preserving the data associated with that pixel. This  effectively de-lenses the data $ T(x+\nabla \phi(x))+n(x)$ but produces observations on an irregular grid. In particular, one may switch to the lensed coordinates $y = x+\nabla\phi(x)$ so that
\begin{align}
\nonumber
\underbrace{(x +\nabla \phi(x),\, \text{ data}(x))}_{\text{(pixel, data) tuple}} & = (y, \, T(y) + \tilde n(y)) 
\end{align}
where $\tilde n(x+\nabla\phi(x))=n(x)$.  We now embed the irregularly sampled data $(y, \, T(y) + \tilde n(y))$ into a high resolution grid by nearest neighbor interpolation. The points $y$ which do not get assigned an observation $T(y)+\tilde n(y)$ under the interpolation we consider to be masked. 
Figure \ref{embed} illustrates this situation. The left hand plot shows the irregularly sampled data $(x +\nabla \phi(x),\, \text{ data}(x))$ and the right hand plot shows the grid embedding. The filled dots represent observations of $T(y) + \tilde n(y)$ whereas the empty dots correspond to a masked observation of $T(y)$. 
Finally we extend the definition of $\tilde n(y)$ to have infinite variance over the masked region, whereby producing data $T(y)+\tilde n(y)$ over a dense regular grid in $y$.




\begin{figure}[H]
%{\includegraphics[height=2in]{Graphics/embed1.eps}}%
{\includegraphics[height=2.5in]{Graphics/embed2.eps}}%
{\includegraphics[height=2.6in]{Graphics/embed3.eps}}
\caption{The embedding of the lensed grid into a high resolution regular grid. The open circles represent masking.}
\label{embed}
\end{figure}


As a intermediate step in producing a sample from  $P(\widetilde T |  \phi,\text{\rm data})$ we produce a conditional  sample of $T(y)$ given the observations $T(y) + \tilde n(y)$. The difficulty of this step is that $\tilde n(y)$ is non-homogeneous noise (from the masking at the minimum) and therefore it is not decorrelated by the Fourier transform.
We adapted a new method developed by Wandelt, Elsner and Lavaux for Gaussian conditional expectation and sampling when the signal is diagonalized in harmonic space that the noise is diagonalized in pixel space. This method works particularly well for observations with large amounts of irregular masking, as in our case. The algorithm utilizes a messenger field which effectively behaves as a latent---signal plus white noise---model.



\begin{algorithm}[H]
\small
\caption{Iterative message passing sampler  $P(T | T + \tilde n)$}
\label{IMP}
\begin{algorithmic}[1]
\State Set cooling schedule $\lambda^1\geq  \ldots \geq \lambda^n = 1$. 
\State Define $Z^{1}(y), \ldots, Z^{n}(y)$ to be independent realizations of standard Gaussian noise, in the pixel domain, so that $E \bigl[ Z^{k}(y)Z^{k}(y^\prime)\bigr] = \delta_{yy^\prime}$.
\State Define $W^{1}_l, \ldots, W^{n}_l$ to be independent realizations of Gaussian white noise, in the Fourier domain, so that $E\bigl[ W^{n}_l W^{n\,*}_{l^\prime}\bigr] = \delta_{l - l^\prime}$.
\State Initialize the fields $M^0(y)$ and $T^0(y)$  to be zero.
\State Decompose the noise variance $\text{var}(\tilde n(y))$ into a homogeneous part $\bar\sigma^2$ and a non-homogeneous part $\tilde\sigma_y^{2}$ so that $\text{var}(\tilde n(y)) = \bar\sigma^2 + \tilde\sigma_y^{2}$.
\State Recursively compute $T^k, M^k$ for $k=1, \ldots, n$ as follows:
\begin{align}
M^{k}(y) &:= \bigl[T(y) + \tilde n(y) \bigr]\frac{\lambda^{k}\bar\sigma^{2}}{\lambda^{k} \bar\sigma^2 + \tilde \sigma_y^2} + T^{k-1}(y)\frac{\tilde\sigma_y^{2}}{\lambda^k\bar\sigma^2 + \tilde \sigma_y^2}  + Z^{k}(y) \frac{1}{\sqrt{1/(\lambda^k\bar\sigma^{2}) + 1/\tilde\sigma_y^{2}}} \\
T^{k}_l &:= M_l^{k}\frac{C^{TT}_l}{C^{TT}_l + dy \, \lambda^k\bar\sigma^2} + W^{k}_l
\frac{1}{\sqrt{1/C^{TT}_l + 1/(dy \lambda^k\bar\sigma^2)}} 
\end{align}
where  $dy$ denotes the pixel grid area. 
\end{algorithmic}
\end{algorithm}


% Now, to produce a conditional sample $ P(\widetilde T |  \phi,\text{\rm data})$ we first produce a conditional sample sample from $T(y)$ given $T(y)+\tilde n(y)$ using Algorithm \ref{IMP}, then lense the sample of $T(y)$ to produce $\widetilde T(x)$. The following algorithm desribes this in detail

The following algorithm now describes how to use Algorithm \ref{IMP} to produce a sample from $ P(\widetilde T |  \phi,\text{\rm data})$

\begin{algorithm}[H]
\small
\caption{Sampling from $P(\widetilde T |  \phi,\text{\rm data})$}
\label{ith step of HMC}
\begin{algorithmic}[1]
\State Embedded the de-lensed pixel/data pairs  $(x+\nabla\phi(x), \text{data}(x))$, as illustrated in Figure \ref{embed}, into observations of the form $(y, T(y)+\tilde n(y))$ where $y$ ranges over a high resolution regular grid.
\State Use Algorithm \ref{IMP} to produce a sample $T(y)\sim P(T(y)| T(y)+\tilde n(y))$
\State Set $\widetilde T(x) = T(x+\nabla\phi(x))$.
\end{algorithmic}
\end{algorithm}




{\em Remark:} At present this algorithm only working with no, or very small, beam. ..
 still represents independent Gaussian noise at the new pixel locations (only if $N$ was white noise to begin with...beams are another story). Therefore under the new coordinate system, $y$, the observations are simply unlensed CMB  corrupted with independent noise.
 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simulation examples}
\label{Section: simulation examples}


Be sure to give some of the exact details...like the fact that we initialize with gradient steps and that we do three hmc steps for each gibbs pass.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Concluding remarks}

What we have accomplished

What needs to be done.
\begin{itemize}
 \item High resolution embedding does not scale to Planck (the native resolution is $\sim$10 million pixels)
\item Approximate Gaussian conditional sampling on Planck data resolution (using previous $\phi^{i}$ to seed a conj gradient?)
\item Need fast anti-lensing operations in frequency space to compute $A^q(x)$ and $B(x)$ that does not requires high res $\tilde T$
\item Incorporate spectral density uncertainty
\item Incorporate polarization
\item A full solution to this problem would handle non-stationary noise, non-stationary beam, cut sky or masking, In this paper we ... one of the main obstacles for the Bayesian lensing problem is ...
\item Possible application to lensing of the 21 cm where the taylor approximation of lensing doesn't lend itself to a quadratic estimate
\end{itemize}




\bibliography{refs}





%%%%%%%%%%%%%%%%%%%%
%
%  appendix
%
%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{}
%\section{Proofs}

Before we proceed to the proofs we say a few words regarding notation.
Firstly, we do not differentiate, notationally, from the case of smooth random field with periodic boundary conditions defined on $(-L/2, L/2]^2$ and the case where $L\rightarrow \infty$ so that the Fourier series $\sum_{l \in \frac{2\pi}{L}\Bbb Z }   e^{i x\cdot l}  f_l \frac{2\pi/ L}{2\pi} $ converges to the continuous Fourier transform $\int_{\Bbb R^2}  e^{i x\cdot l}  f_l \frac{dl}{2\pi} $. %Indeed,  $\int_{\Bbb R^2}  e^{i x\cdot l}  f_l \frac{dl}{2\pi} $ is simply equivalent notation for the Riemann sum, with $L$ near infinity.
For example, at times we will refer to an infinitesimal area element $dl$ or $dk$ in Fourier space, which simply equals $\frac{2\pi}{L}$ for large $L$. In this case $\delta_l$ denotes a discrete dirac delta function which we equate with $1/dk$ when $l=0$ and zero otherwise. 
Secondly, for any function $f(x)$ let $f^\phi(x) = f(x-\nabla \phi(x))$ denote anti-lensing of $f$ and $f^\phi_l$ denote the Fourier transform of  $f^\phi(x)$.





\begin{proof}[{ Proof of Claim \ref{grad claim}}]
Since $\tilde T$ is sufficient for the unknown $\phi$ we have that 
\begin{align*}
P(\phi|\widetilde T, \text{\rm data}) &= P(\phi|\widetilde T)\propto P(\widetilde T|\phi)P(\phi).
\end{align*}
Since $\phi(x)$ is an isotropic random field with spectral density $C_l^{\phi\phi}$ we have that $E(\phi^{\phantom{*}}_l \phi_{l^\prime}^*) = \delta_{l - l^\prime}C_l^{\phi\phi}$. 
Therefore  $E(\phi_l^{\phantom{*}}\phi_l^*) = \delta_{0}C_l^{\phi\phi}$ and $E(\phi_l\phi_{l}) = 0$ one gets that the random variables $\re \phi_l$, $\im \phi_l$ are independent $\mathcal N(0, \frac{1}{2}\delta_0 C_l^{\phi\phi})$ for each fixed $l$.
Moreovoer  $\phi(x)$ takes values in $\Bbb R$ so that  $\phi_l = \phi_{-l}^*$.  This implies (\textcolor{red}{what exactly implies what here? clearly the moments don't tell us exactly that they are independent since $E(z w^*)=0$ could happen when $w = z^*$, and $(\re z, \im z)^t\sim \mathcal N(0,\sigma^2 I)$. But I think this is the only case where $E(zw^*)=0$ when we know $z$ and $w$ are marginally Gaussian.}) that $\phi_l$ and are independent random variables over all $l$ which are restricted to the a Hermitian half of the Fourier grid, denoted $\Bbb H$. In particular,  if we exclude the zero frequency $l = 0$ we get
\begin{align}
\log P(\phi) - c_1 &=  - \frac{1}{2}\sum_{k \in \Bbb H\setminus \{0 \}}  \left[\frac{(\re \phi_k)^2}{\frac{1}{2}\delta_0 C_k^{\phi\phi}} +  \frac{(\im \phi_k)^2}{\frac{1}{2}\delta_0 C_k^{\phi\phi}} \right] = - \frac{1}{2}\int_{\Bbb R^2} \frac{|\phi_k|^2}{C_k^{\phi\phi}} dk \label{dr1} \\
\log P(\widetilde T| \phi) - c_2 &=  - \frac{1}{2}\sum_{k \in \Bbb H\setminus \{0 \}}  \left[\frac{(\re \widetilde T_k^\phi)^2}{\frac{1}{2}\delta_0 C_k^{TT}} +  \frac{(\im \widetilde T_k^\phi)^2}{\frac{1}{2}\delta_0 C_k^{TT}} \right] = - \frac{1}{2}\int_{\Bbb R^2} \frac{\bigl|\widetilde T_k^\phi\bigr|^2}{C_k^{TT}} dk \label{dr2}
\end{align}
where $c_1$ and $c_2$ are constants and  $\widetilde T^\phi(x)\equiv \widetilde T(x-\nabla \phi(x))$.
% -------- extra detail
% The integral approximation then becomes
% \[ \log P(\phi) = \text{constant} - \pi \int \left[\frac{(\re \phi_l)^2}{C_l^{\phi\phi}} +  \frac{(\im \phi_l)^2}{ C_l^{\phi\phi}} \right] \frac{dl}{2\pi} \]
% -------- extra detail
% Therefore
% \begin{align*}
% \frac{\partial}{\partial \re \phi_l}\log P(\phi)  &= -2 \frac{\re \phi_l}{\delta_0 C_l^{\phi\phi}} \\
% \frac{\partial}{\partial \im \phi_l}\log P(\phi)  &= -2 \frac{\im \phi_l}{\delta_0 C_l^{\phi\phi}}
% \end{align*}
Taking derivatives in (\ref{dr1}) gives
\begin{equation}
\label{grad of prior}
\frac{\partial}{\partial \phi_l}\log P(\phi) = - 2(dl) \frac{\phi_l}{C^{\phi\phi}_{l}}.
\end{equation}
Taking derivatives in (\ref{dr2}) gives
\begin{align}
\frac{\partial}{\partial \re\phi_l}\log P(\widetilde T| \phi) 
% <----- extra detail
%&=  - dk\sum_{k \in \Bbb H} 2 \re \Bigl( \frac{\partial \widetilde T_k^\phi}{\partial \re\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \Bigr)\\
&= - \re \int_{\Bbb R^2} \frac{\partial \widetilde T_k^{\phi}}{\partial \re\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk \label{ll1}\\
\frac{\partial}{\partial \im\phi_l}\log P(\widetilde T| \phi) 
% <----- extra detail
%&=  - dk\sum_{k \in \Bbb H}2 \re \Bigl( \frac{\partial \widetilde T_k^\phi}{\partial \im\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \Bigr) \\
&=  -\re \int_{\Bbb R^2}\frac{\partial \widetilde T_k^{\phi}}{\partial \im\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \, dk \label{ll2}.
\end{align}
Taking linear combinations of the two equalities in Lemma \ref{partialconj} we get
\begin{align}
\frac{\partial \widetilde T^\phi_k}{\partial \re \phi_{l}}  &= 
\frac{1}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi_{l}} + \frac{1}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi^*_{l}}
=
\frac{dk}{2 \pi}  \sum_{q=1,2} il_q\left\{ [(\nabla^q\widetilde T)^\phi]_{k-l}  -   [(\nabla^q\widetilde T)^\phi]_{k+l}  \right\}\\
\frac{\partial \widetilde T^\phi_k}{\partial \im \phi_{l} }  &= 
\frac{-i}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi_{l}} + \frac{i}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi^*_{l}}
=
\frac{ dk}{2 \pi} \sum_{q=1,2} l_q\left\{ -[(\nabla^q\widetilde T)^\phi]_{k-l}  -   [(\nabla^q\widetilde T)^\phi]_{k+l}  \right\}.
\end{align}
Now the above two equations establish, by Lemma \ref{forreal}, that  both integrals $\int_{\Bbb R^2} \frac{\partial \widetilde T_k^\phi}{\partial \re\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk$ and $\int_{\Bbb R^2}\frac{\partial \widetilde T_k^\phi}{\partial \im\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \, dk$ are real
which implies
\begin{align*}
 \frac{\partial}{\partial \phi_l}\log P(\widetilde T| \phi) &= -\int_{\Bbb R^2} \frac{\partial \widetilde T_k^\phi}{\partial \phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk  \\
 &= -\frac{dk}{ \pi} \sum_{q=1,2} il_q \int_{\Bbb R^2}  [(\nabla^q\widetilde T)^\phi]_{k+l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk \\
 &= -  i 2 (dk) \sum_{q=1,2} l_q \int_{\Bbb R^2}  [(\nabla^q\widetilde T)^\phi]_{k+l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,\frac{dk}{2\pi} \\
 & = -  i 2 (dk) \sum_{q=1,2} l_q \int_{\Bbb R^2} e^{-i x\cdot l} A^q(x) \, B(x)  \,\frac{dx}{2\pi},\quad\text{by Lemma \ref{conv}}
 \end{align*}
 where $A^q(x) \equiv (\nabla^q\widetilde T)^\phi(x)$ and $B_k\equiv (\widetilde T_k^{\phi})^* / C_k^{TT}$.
\end{proof}
% end proof



%%%%%%%%%%%%%%% lemma
\begin{lemma} 
\label{partialconj}
\begin{align}
\frac{\partial \widetilde T^\phi_k}{\partial \phi_{ l} }  &= \frac{  dk}{\pi}\sum_{q=1,2} -il_q[(\nabla^q\widetilde T)^\phi]_{k+l} \\
\frac{\partial \widetilde T^\phi_k}{\partial \phi^*_{ l} }  & = \frac{ dk }{\pi}\sum_{q=1,2} \phantom{-}\!\!il_q [(\nabla^q\widetilde T)^\phi]_{k-l}
\end{align}
where $\nabla^q \widetilde T\equiv \frac{\partial \widetilde T}{\partial x_q}$.
\end{lemma}
\begin{proof}
First notice
\begin{align}
\label{partial1}
\frac{\partial}{\partial \re \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q}  &= \int_{\Bbb R^2} i  k_q e^{ix \cdot k} \frac{\partial \phi_k}{\partial \re \phi_l }  \frac{d k}{2\pi} 
=\left[i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  \right] \frac{d k}{2\pi}   \\
\label{partial2}
\frac{\partial}{\partial \im \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q}  &= \int_{\Bbb R^2} i  k_q e^{ix \cdot  k} \frac{\partial \phi_ k}{\partial \im \phi_{l} }  \frac{d k}{2\pi} 
=\left[-  l_q e^{ix \cdot  l}  -  l_q e^{-i x \cdot  l}  \right] \frac{d k}{2\pi}.  
\end{align}
This implies
\begin{align}
\nonumber \frac{\partial \widetilde T^\phi_k}{\partial \phi_{ l} } 
&=  \frac{\partial}{\partial  \phi_{ l} } \int_{\Bbb R^2}  e^{-ix \cdot k}\widetilde T(x+\nabla \phi(x))\frac{dx}{2\pi} \\
\nonumber &= \sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ \frac{\partial}{\partial \re \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q} +i  \frac{\partial}{\partial \im \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q}\right]\frac{dx}{2\pi}\\
% <-----extra detail
%  &= \frac{d k}{2\pi} \sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  + i(-  l_q e^{ix \cdot  l}  -  l_q e^{-i x \cdot  l})  \right]\frac{dx}{2\pi} \\
% &= \frac{d k}{2\pi}  \sum_{q=1,2} i l_q \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ e^{ix \cdot  l}  -  e^{-i x \cdot  l}  -  e^{ix \cdot  l}  -  e^{-i x \cdot  l} \right]\frac{dx}{2\pi}\\
% <-----
\nonumber &= \sum_{q=1,2}\frac{ - il_q  dk}{\pi}  \int_{\Bbb R^2} e^{- ix \cdot (k+l)} \nabla^q\widetilde T(x+\nabla \phi(x))\frac{dx}{2\pi},\quad\text{by (\ref{partial1}) and (\ref{partial2})} \\
&=\sum_{q=1,2}\frac{ - il_q dk}{\pi} [(\nabla^q\widetilde T)^\phi]_{k+l}
\end{align}
Similarly 
\begin{align}
\frac{\partial \widetilde T^\phi_k }{\partial \phi^*_l } 
% <-----extra detail
% &= \frac{d k}{2\pi}\sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  - i(-  l_q e^{ix \cdot  l}  -  l_q e^{-i x \cdot  l})  \right]\frac{dx}{2\pi} \\
% &= \frac{d k}{2\pi} \sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  + i  l_q e^{ix \cdot  l}  +i  l_q e^{-i x \cdot  l}  \right]\frac{dx}{\pi} \\
&= \sum_{q=1,2}  \frac{i l_qd k}{\pi}\int_{\Bbb R^2} e^{-ix \cdot (k-l)} \nabla^q\widetilde T(x+\nabla \phi(x))\frac{dx}{\pi} \\
% <-------
 &=\sum_{q=1,2}\frac{ il_q dk}{\pi} [(\nabla^q\widetilde T)^\phi]_{k-l}.
\end{align}
\end{proof}





%%%%%%%%%%%% lemma
\begin{lemma} 
\label{forreal}
If $A(x)$ and $B(x)$ are real scalar fields then  the two  integrals,  $\int_{\Bbb R^2} i\bigl\{ A_{k-l}  -   A_{k+l}  \bigr\} B^*_k dk$ and  $\int_{\Bbb R^2}\bigl\{  A_{k-l}  +    A_{k+l}   \bigr\} B^*_k dk$, are both real numbers.
\end{lemma}


\begin{proof}
By a simple change of variables it is clear that 
$\int_{\Bbb R^2} \left(i\bigl\{ A_{k-l}  -   A_{k+l}  \bigr\} B^*_k\right)^* dk = \int_{\Bbb R^2} i\bigl\{ A_{k^\prime-l} - A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime$ and $\int_{\Bbb R^2} \left(\bigl\{  A_{k-l}  +    A_{k+l}   \bigr\} B^*_k \right)^* dk  = \int_{\Bbb R^2} \bigl\{ A_{k^\prime-l} + A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime$.

% \begin{align*}
% \int_{\Bbb R^2} \left(i\bigl\{ A_{k-l}  -   A_{k+l}  \bigr\} B^*_k\right)^* dk &= \int_{\Bbb R^2} -i\bigl\{ A_{-k+l}  -   A_{-k-l}  \bigr\} B_{k} dk \\
% &= \int_{\Bbb R^2} -i\bigl\{ A_{k^\prime+l}  -   A_{k^\prime-l}  \bigr\} B_{-k^\prime} dk^\prime \\
% &= \int_{\Bbb R^2} i\bigl\{ A_{k^\prime-l} - A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime
% \end{align*}

% \begin{align*}
% \int_{\Bbb R^2} \left(\bigl\{  A_{k-l}  +    A_{k+l}   \bigr\} B^*_k \right)^* dk &= \int_{\Bbb R^2} \bigl\{ A_{-k+l}  +   A_{-k-l}  \bigr\} B_{k} dk \\
% &= \int_{\Bbb R^2} \bigl\{ A_{k^\prime+l}  +   A_{k^\prime-l}  \bigr\} B_{-k^\prime} dk^\prime \\
% &= \int_{\Bbb R^2} \bigl\{ A_{k^\prime-l} + A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime
% \end{align*}
\end{proof}



%%%%%%%%%%%% lemma
\begin{lemma} 
\label{conv}
If $A(x)$ and $B(x)$ are real scalar fields then  $\int_{\Bbb R^2} A_{k+l}  B^*_k \frac{dk}{2\pi}= \int_{\Bbb R^2} e^{-ix\cdot l} A(x)B(x)\frac{dx}{2\pi}$.
\end{lemma}

\begin{proof} 
\begin{align*}
\int_{\Bbb R^2} A_{k+l}  B^*_k \frac{dk}{2\pi} 
& = \int_{\Bbb R^2} \int_{\Bbb R^2} e^{-x\cdot (k+l)} A(x) B_{k}^*  \frac{dx}{2\pi}   \frac{dk}{2\pi} \\
& = \int_{\Bbb R^2}e^{-ix\cdot l} A(x) \left[\int_{\Bbb R^2} e^{-x\cdot k} B_{k}^*     \frac{dk}{2\pi}\right] \frac{dx}{2\pi}\\
& = \int_{\Bbb R^2}e^{-ix\cdot l} A(x) \left[\int_{\Bbb R^2} e^{x\cdot k} B_{k}     \frac{dk}{2\pi}\right]^* \frac{dx}{2\pi}\\
&= \int_{\Bbb R^2} e^{-ix\cdot l} A(x)B^*(x)\frac{dx}{2\pi}\\
&= \int_{\Bbb R^2} e^{-ix\cdot l} A(x)B(x)\frac{dx}{2\pi},\quad\text{since $B(x)$ is real.}
\end{align*}

\end{proof}




\end{document}

