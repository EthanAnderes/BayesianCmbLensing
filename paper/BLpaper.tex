\documentclass[11pt]{article}

\usepackage{amsmath,amsthm,amssymb,epsf,eucal}
\usepackage{bm} %% Bold Math
\usepackage{graphicx,psfrag}
\usepackage{epstopdf}
\usepackage{verbatim}
\usepackage{enumerate}
\usepackage{multirow}
\usepackage{color}
\usepackage{setspace}
\usepackage{subfigure}
\usepackage{float}
\usepackage{enumerate}


\bibliographystyle{plain}
\addtolength{\textwidth}{.4in} %this number is 2 times the next number
\addtolength{\hoffset}{-.2in}

\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\renewcommand{\labelenumi}{(\roman{enumi})}

\title{Bayesian CMB lensing}
\date{}


\begin{document}
\maketitle

The Bayesian solution to the estimation of CMB lensing  is to generate random draws from the posterior distribution, given the observed data, on the lensing potential and the unlensed (noiseless) CMB field. This approach is attractive for statistical inference since posterior draws are easy to use and to interpret for scientific inference. Moreover, posterior distributions can often be sequentially updated to incorporate additional information from additional data or other experiments.

History of the Bayesian lensing problem.

A full solution to this problem would handle non-stationary noise, non-stationary beam, cut sky or masking, In this paper we ... one of the main obstacles for the Bayesian lensing problem is ...

There are two components to this solution. The first.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Primer on Gibbs parameterization
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Simple two parameter system}
\label{two parameter system}


To motivate our solution to the Bayesian lensing problem we start with a simple two parameter statistical problem.  This system has two unknown parameters $\theta, \phi$ with data given by
\[\text{data} = \theta + \phi + n\]
where $n$ denotes additive noise.  In the Bayesian setting, the posterior distribution is computed as 
\begin{equation}
\label{post1}
 P(\theta,\phi|\text{data})\propto P(\text{data}|\theta, \phi) P(\theta,\phi) 
 \end{equation}
where $P(\text{data}|\theta, \phi)$ denotes the likelihood of the data given  $\theta, \phi$ and $P(\theta,\phi)$ denotes the  prior on $\theta, \phi$. 
The Gibbs sampler is a widely used algorithm for generating (asymptotic) samples from  $P(\theta, \phi|\text{data})$ \textcolor{red}{[add citations]}. The algorithm generates a Markov chain of parameter values $(\theta^{1}, \phi^{1}), (\theta^{2}, \phi^{2}),\ldots$ generated by iteratively sampling from the conditional distributions:
\begin{align*}
\theta^{i+1} &\sim P(\theta|\phi^i,\text{data})\\
\phi^{i+1}   &\sim P(\phi|\theta^{i+1},\text{data}).
\end{align*}
A useful heuristic for determining the convergence rate of a Gibbs chain is the extent to which the two parameters $\theta$ and $\phi$ are dependent in $P(\phi, \theta|\text{data})$. A highly dependent posterior $P(\theta, \phi|\text{data})$ leads to a slow Gibbs chain, near independence leads to a fast Gibbs chain. Indeed, exact independence gives a sample of the posterior after one Gibbs step.  A technique for accelerating the convergence of a Gibbs sampler is to find a  re-parameterization of $\theta$ and $\phi$ in a way which makes the posterior less dependent. In the remainder of this section we discuss a specific re-parameterization which, by analogy, can be applied to Bayesian lensing.


The relevant situation for Bayesian lensing is the case that $\theta$ and $\phi$ are highly negatively correlated in $P(\theta, \phi|\text{data})$.  This motivates re-parameterizing $(\theta,\phi)$ to $(\widetilde \theta, \phi)$ where $\widetilde \theta \equiv \theta + \phi$ so that
\begin{align*}
\text{data} &= \widetilde \theta + n.
\end{align*}
In the statistics literature,  $(\theta, \phi)$ is commonly referred to as an {\bf ancillary parameterization} whereas $(\widetilde \theta, \phi)$ is referred to as a {\bf sufficient parameterization} \textcolor{red}{[add citations]}. Figure  \ref{fastslowGibbs} illustrates the difference between an ancillary versus sufficient posterior distribution for our simple two parameter model. The left plot shows the posterior density contours for the ancillary parameterization $(\theta, \phi)$, along with 40 steps of a Gibbs sampler.  Conversely, the right plot shows the posterior density contours for the sufficient chain $(\widetilde \theta, \phi)$ with 40 Gibbs steps. Notice that negative correlation  in the ancillary parameterization manifests in near independence for the sufficient chain.  Indeed, the slower the ancillary chain the faster the sufficient chain and vice-versa. 
\begin{figure}[H]
\label{fastslowGibbs}
\includegraphics[height=2.0in]{Graphics/theta.pdf}\includegraphics[height=2.0in]{Graphics/thetatilde.pdf}
\caption{{\em Left:} density contours of the {\bf ancillary} chain $P(\theta, \phi|\text{data})$ with 40 steps of a Gibbs sampler. {\em Right:} density contours of the {\bf sufficient}  chain $P(\widetilde \theta, \phi|\text{data})$ with 40 steps of a Gibbs sampler.}
\end{figure}
 






%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ancillary and sufficient parameters for the lensed CMB}

The analog to the two parameter ancillary problem, $\text{data} = \theta +\phi +n$,  in Section \ref{two parameter system} to the lensed CMB is 
\[\text{data}(x) = T(x+\nabla \phi(x)) + n(x)\]
where the unlensed  CMB temperature field $T$ and the lensing potential $\phi$ are the two unknown parameters. In this chain the Gibbs sampler proceddes in the usual way:
\begin{align}
T^{i+1}&\sim P(T |  \phi^{i},\text{data})\\
\phi^{i+1}&\sim P(\phi | T^{i+1},  \text{data}).
\end{align}
Sampling from $P(T |  \phi^{i},\text{data})$ is simply a Gaussian random field prediction problem since conditioning on $\phi^i$ models the data as
\[ \text{data}(x) = T(\!\!\underbrace{x+\nabla\phi^i(x)}_\text{\tiny known obs locations}\!\!) + n(x).\]
In otherwords, he data is a noisy version of  $T$ observed on an irregular grid. 
Conversly, when sampling from $P(\phi |  T^{i+1},\text{data})$ the data is of the form
\[ \text{data}(x) = \underbrace{T^{i+1}}_{\text{\tiny known}}(x+\nabla\phi(x)) + n(x). \]
Both of these conditionals make the Gibbs very slow to converge. The case is esasterbated in the situation the noise level is small. For example, in the second conditional, if $T^{i+1}$ is knownw and fixed, the extent of possible  $\phi$'s which are possible under $P(\phi|T^{i+1},\text{data})$  is very small compared to the possible $\phi$'s  in $P(\phi, T| \text{data})$ when $T$ is allowd to vary. 
This suggests a highly dependent posteror $P(\phi, T| \text{data})$. 
This was also noitce by \textcolor{red}{[Cite Lewis and Challanore]} for the first conditnioal.

This clearly modivates attempting to quantify a  sufficient parameterization. The analog to the sufficient toy problem, $\text{data} = \widetilde\theta + n$,  in Section \ref{two parameter system} to the lensed CMB is 
\[\text{data}(x) = \widetilde T(x) + n(x)\]
where now $\widetilde T$ denotes the lensed CMB tempuruature field with no noise or beam.
The suffient chain then procceeds as
\begin{align}
\widetilde T^{i+1}&\sim P(\widetilde T |  \phi^{i},\text{data})\\
\phi^{i+1}&\sim P(\phi | \widetilde T^{i+1},  \text{data}).
\end{align}
In the following two ections we discuss these two conditional in detail.


\section{$\widetilde T^{i+1}\sim P(\widetilde T |  \phi^{i},\text{data})$}


\section{$\phi^{i+1}\sim P(\phi | \widetilde T^{i+1},  \text{data})$}


\end{document}

