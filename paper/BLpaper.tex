\documentclass[noinfoline]{imsart}


\usepackage{bm}
\usepackage{graphics,epsfig,rotate,lscape,graphicx,amsmath,amsthm,amssymb,float,amsfonts,amsbsy,hyperref,delarray,sectsty,amsfonts,amscd,pifont}
\usepackage{color,multirow}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage{enumerate}

\usepackage{geometry}
\geometry{letterpaper,left=1.2in,right=1.2in,top=1.2in,bottom=1.1in}

\bibliographystyle{plain}


\newtheorem{claim}{Claim}
\newtheorem{proposition}{Proposition}
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{conjecture}{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{notation}{Notation}
\renewcommand{\labelenumi}{(\roman{enumi})}
\newcommand{\bx}{\boldsymbol x}
\newcommand{\by}{\boldsymbol y}
\newcommand{\bl}{\boldsymbol l}
\newcommand{\bk}{\boldsymbol k}
\newcommand{\re}{\text{\,\rm re}}
\newcommand{\im}{\text{\,\rm im}}



% --------------------------------------
\begin{document}



\begin{frontmatter}
\title{Bayesian estimates of CMB gravitational lensing}
\runtitle{Bayesian CMB lensing}
\begin{aug}
  \author{\fnms{Ethan}  \snm{Anderes}\corref{}\thanksref{t1}\thanksref{a}}
  \author{\fnms{Benjamin} \snm{Wandelt}\thanksref{b}}
  \and
  \author{\fnms{Guilhem}  \snm{Lavaux}\thanksref{b}}%
  \ead[label=u1,url]{https://github.com/EthanAnderes/BayesianCmbLensing.git}
  \address[a]{Department of Statistics, University of California, Davis CA 95616, USA.}
  \address[b]{Institut d' Astrophysique de Paris. }
  \thankstext{t1}{Research supported by: NSF DMS-1007480, NSF CAREER DMS-1252795}
  \thankstext{t2}{Code available at \printead{u1}}
  \runauthor{E. Anderes et al.}
\end{aug}

\begin{abstract} 
Gravitational lensing studies have become a powerful probe of dark matter, the invisibility of which provides the main obstacle for detection. In particular, the ground based Atacama Cosmology Telescope (ACT)  and  the South Pole Telescope (SPT) have mapped the  cosmic microwave background (CMB)  at such unprecedented resolution as too allow a detection of weak gravitational lensing from the CMB alone.
The CMB shows a picture of radiation fluctuations frozen at the instant the universe became transparent. Estimating the gravitational lensing of the CMB  is important for two reasons. First, if the CMB is mapped at a sufficient resolution one can use weak lensing estimates to construct a map of dark matter in the sky.
Second,  weak lensing estimates  can be used, in principle,  to un-distort the observed lensed CMB and construct the original unlensed CMB radiation fluctuations. Both of these maps,  the unlensed CMB radiation field and the dark matter field, are deep probes into the nature of cosmology and cosmic structure. 
\end{abstract}

\begin{keyword}
\kwd{CMB}
\kwd{gravitational lensing}
\kwd{Bayesian}
\kwd{Gibbs sampler}
\end{keyword}

\end{frontmatter}





 Over the past few years the data from ground based telecopse  (ACT, SPT and Bicep2) and  the Plank satellite and have resulted in unprecedicented detection of weak lensing of the CMB (see Das et al. (2011) and van Engelen  et al. (2012), Plank).  These observations are constraining cosmological models of gravity waves, dark matter and dark energy. The state-of-the-art estimator of CMB lensing, the quadratic estimator developed by Hu and Okomoto (2001, 2002), works in part through a delicate cancelation of terms in an infinite Taylor expansion of the lensing effect on the CMB. The effect of this cancelation is particularly sensitive to foreground contaminants and sky masking, which  if not fully accounted for,  limit  the statistical inference obtainable from these studies.  

 Possibly the most promising alternative to the quadratic estimator, is Bayesian lensing. Indeed, Bayesian techniques applied to the lensed CMB observations have the potential for drastically changing the way lensing is estimated and used for inference. The Bayesian goal in this case is to generate random draws from the posterior distribution, given the observed data, on the lensing potential and the unlensed (noiseless) CMB field. This approach is attractive for statistical inference since posterior draws are easy to use and to interpret for scientific inference. Moreover, posterior distributions can often be sequentially updated to incorporate additional information from additional data or other experiments.
 From the geometry of weak lensing, most of the lensing power comes from matter at a redshift $z\approx 2$. At these distances the matter distribution is well approximated by linear theory which predicts the matter density fluctuations are nearly Gaussian. Moreover, the unlensed CMB, denoted  $T$, is also extremely close to Gaussian.  From a statistical perspective, this is a perfect scenario for Bayesian methods, in that both the observations and the unknown lensing potential are {\em physically predicted} to be Gaussian random fields.  Moreover, the structure of the lensing operation gives a compelling case for MCMC posterior sampling techniques.
 


History of the Bayesian lensing problem.

A full solution to this problem would handle non-stationary noise, non-stationary beam, cut sky or masking, In this paper we ... one of the main obstacles for the Bayesian lensing problem is ...

There are two components to this solution. The first.




%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Weak lensing primer and a Bayesian challenge}
\label{primer}

This section describes the  basics of CMB lensing and Bayesian estimates of lensing.  The effect of weak lensing is to simply remap the CMB, preserving surface brightness.   Up to leading order, the remapping  displacements are given by $\nabla \phi$, where $\phi$ denotes a lensing potential and is the planer projection of a three dimensional gravitational potential (see Dodelson, S. (2003), for example). Therefore the lensed CMB can  be written $T(\bx + \nabla \phi(x))$ where $T(x)$ denotes the unlensed CMB temperature fluctuations projected to the observable sky.
The goal of weak lensing surveys is to use the lensed observations $T(x + \nabla \phi(x))$  (with additional noise) to  estimate $\phi$ or the spectral density of $\phi$.  In the full sky, $x$ represents an observational direction on the unit sphere. However, we will be focusing on the small angle limit  so that $x$ can be modeled as a variable in $ \Bbb R^2$.  The Einstein principle along with properties of quantum mechanics predicts that $T(x)$   is a Gaussian  isotropic random field. These properties translate to  the independence of the Fourier transform of $T$ across different frequencies.
 %Taking a Fourier transform of $T$ translates to independence under the Fourier transform. 
 However, for a fixed lensing potential $\phi$, the lensed CMB  becomes non-isotropic, which leads to a correlation in the Fourier transform across different frequencies. The quadratic estimator   takes advantage of this correlation and uses weighted sums of Fourier  cross products to unbiasedly (up to leading order) estimate the lensing potential.
The quadratic estimator is derived under the assumption that the observed lensed CMB field is contaminated by additive noise and an instrumental beam. Throughout this proposal we let  $T^\text{\tiny obs}(x)$ denote the observed CMB field  with  noise (denoted $n(x)$) so that  
\[ \text{data}(x)= T(x + \nabla \phi(x))+ n(x)
%+\int T(\bs y+\nabla \phi(\bs y)) \varphi(\bs x - \bs y) \, d\bs y.
 \]
The quadratic estimator is based on a first order Taylor approximation in $\nabla \phi $ on the lensed CMB field:
$T(x+\nabla \phi())= T(x) + \nabla \phi(x)\cdot \nabla T(x)+ O(\phi^2)$. 
In Anderes and Paul (2012) they showed that this estimator is essentially  a generalized least square regression estimator obtained by stacking the cross product of the Fourier transform separated at a certain lag.



There has been active interest in devloping a Bayesian estimator of the lensing potential $\phi$ jointly with the unlensed CMB $T$ given the data. A very natural approach to generating posterior samples is to 
In the ancillary Gibbs chain proceeds in the usual way:
\begin{align}
T^{i+1}&\sim P(T |  \phi^{i},\text{data})\\
\phi^{i+1}&\sim P(\phi | T^{i+1},  \text{data}).
\end{align}
Sampling from $P(T |  \phi^{i},\text{data})$ is simply a Gaussian random field prediction problem since conditioning on $\phi^{i}$ models the data as
\[ \text{data}(x) = T(\!\!\underbrace{x+\nabla\phi^{i}(x)}_\text{\tiny known obs locations}\!\!) + n(x).\]
In otherwords, he data is a noisy version of  $T$ observed on an irregular grid. 
Conversly, when sampling from $P(\phi |  T^{i+1},\text{data})$ the data is of the form
\[ \text{data}(x) = \underbrace{T^{i+1}}_{\text{\tiny known}}(x+\nabla\phi(x)) + n(x). \]
Both of these conditionals make the Gibbs very slow to converge. The case is exacerbated in the situation when noise level is small. For example, in the second conditional, if $T^{i+1}$ is known and fixed, the extent of the  $\phi$'s which are possible under $P(\phi|T^{i+1},\text{data})$  is very small compared to the possible $\phi$'s  in $P(\phi, T| \text{data})$ when $T$ is allowed to vary. 
This suggests a highly dependent posterior $P(\phi, T| \text{data})$. 
This was also noticed by \textcolor{red}{[Cite Lewis and Challanore]} for the first conditional.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Two parameter analogy}
\label{two parameter system}


To motivate our solution to the Bayesian lensing problem we start with a simple two parameter statistical problem.  This system has two unknown parameters $ t, \varphi$ with data given by
\[\text{data} =  t + \varphi + n\]
where $n$ denotes additive noise.  In the Bayesian setting, the posterior distribution is computed as 
\begin{equation}
\label{post1}
 P( t,\varphi|\text{data})\propto P(\text{data}| t, \varphi) P( t,\varphi) 
 \end{equation}
where $P(\text{data}| t, \varphi)$ denotes the likelihood of the data given  $ t, \varphi$ and $P( t,\varphi)$ denotes the  prior on $ t, \varphi$. 
The Gibbs sampler is a widely used algorithm for generating (asymptotic) samples from  $P( t, \varphi|\text{data})$ \textcolor{red}{[add citations]}. The algorithm generates a Markov chain of parameter values $( t^{1}, \varphi^{1}), ( t^{2}, \varphi^{2}),\ldots$ generated by iteratively sampling from the conditional distributions:
\begin{align*}
 t^{i+1} &\sim P( t|\varphi^{i},\text{data}) \\
\varphi^{i+1}   &\sim P(\varphi| t^{i+1},\text{data}).
\end{align*}
A useful heuristic for determining the convergence rate of a Gibbs chain is the extent to which the two parameters $ t$ and $\varphi$ are dependent in $P(\varphi,  t|\text{data})$. A highly dependent posterior $P( t, \varphi|\text{data})$ leads to a slow Gibbs chain, near independence leads to a fast Gibbs chain. Indeed, exact independence gives a sample of the posterior after one Gibbs step.  A technique for accelerating the convergence of a Gibbs sampler is to find a  re-parameterization of $ t$ and $\varphi$ in a way which makes the posterior less dependent. In the remainder of this section we discuss a specific re-parameterization which, by analogy, can be applied to Bayesian lensing.


The relevant situation for Bayesian lensing is the case that $ t$ and $\varphi$ are highly negatively correlated in $P( t, \varphi|\text{data})$.  This motivates re-parameterizing $( t,\varphi)$ to $(\widetilde  t, \varphi)$ where $\widetilde  t \equiv  t + \varphi$ so that
\begin{align*}
\text{data} &= \widetilde  t + n.
\end{align*}
In the statistics literature,  $( t, \varphi)$ is commonly referred to as an {\bf ancillary parameterization} whereas $(\widetilde  t, \varphi)$ is referred to as a {\bf sufficient parameterization} \textcolor{red}{[add citations]}. Figure  \ref{fastslowGibbs} illustrates the difference between an ancillary versus sufficient posterior distribution for our simple two parameter model. The left plot shows the posterior density contours for the ancillary parameterization $( t, \varphi)$, along with 40 steps of a Gibbs sampler.  Conversely, the right plot shows the posterior density contours for the sufficient chain $(\widetilde  t, \varphi)$ with 40 Gibbs steps. Notice that negative correlation  in the ancillary parameterization manifests in near independence for the sufficient chain.  Indeed, the slower the ancillary chain the faster the sufficient chain and vice-versa. 
\begin{figure}[H]
\label{fastslowGibbs}
\includegraphics[height=2.0in]{Graphics/theta.pdf}\includegraphics[height=2.0in]{Graphics/thetatilde.pdf}
\caption{{\em Left:} density contours of the {\bf ancillary} chain $P( t, \varphi|\text{data})$ with 40 steps of a Gibbs sampler. {\em Right:} density contours of the {\bf sufficient}  chain $P(\widetilde  t, \varphi|\text{data})$ with 40 steps of a Gibbs sampler.}
\end{figure}
 





	
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Section
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Ancillary and sufficient parameters for the lensed CMB}

We will see that the ancillary parameterization for the lensed CMB problem is extreemely slow, whereas the sufficient parameterization is fast. Indeed, the analog to the Ancillary toy problem can be related to the lensed CMB problem as follows
% \begin{align*}
%   \text{data}(x) &= T(x+\nabla \phi(x)) + n(x) \\
%   &\Updownarrow \\
%   \text{data} &=  t +\varphi + n
% \end{align*}
\begin{align*}
  \text{data}(x) &= T(x+\nabla \phi(x)) + n(x)\quad\text{\bf\em analogous to}\quad
  \text{data} =  t +\varphi + n
\end{align*}
where the unlensed  CMB temperature field $T$ and the lensing potential $\phi$ are the two unknown parameters. Since we have seen in Section \ref{primer} that this Ancillary chain is exceedingly slow this clearly modivates attempting to quantify a sufficient parameterization. The analog to the sufficient toy problem, $\text{data} = \widetilde t + n$,  in Section \ref{two parameter system} to the lensed CMB is 
% \begin{align*}
%   \text{data}(x) &= \widetilde T(x) + n(x) \\
%   &\Updownarrow \\
%   \text{data} &=  \widetilde t + n
% \end{align*}
\begin{align*}
  \text{data}(x) &= \widetilde T(x) + n(x) \quad\text{\bf\em analogous to}\quad
  \text{data} =  \widetilde t + n
\end{align*}
where now $\widetilde T$ denotes the lensed CMB tempuruature field with no noise or beam.
The suffient chain then procceeds as
\begin{align}
\widetilde T^{i+1}&\sim P(\widetilde T |  \phi^{i},\text{data}) \\
\phi^{i+1}&\sim P(\phi | \widetilde T^{i+1},  \text{data}).
\end{align}
In the following two ections we discuss these two conditional in detail.



\subsection{Anti-lensing approximation}


% ----------
\section{Hamiltonian sampler for $P(\phi | \widetilde T,  \text{\rm data})$}




Throuout this paper the Fourier transform of any function $f(x)$ is denoted $f_l$ or $f_k$ so that
$f_l  =  \int_{\Bbb R^2} e^{-i x\cdot l}  f(x)\frac{dx}{2\pi}$ and
$f(x) =  \int_{\Bbb R^2} e^{i x\cdot l}  f_l \frac{dl}{2\pi}$ 
where $l\in \Bbb R^2$ is a two dimensional frequency vector and $x\in \Bbb R^2$  is a two dimensional spatial coordinate.

\begin{claim}
\label{grad claim}
 Under the anti-lensing approximation, that   $\widetilde T(x-\nabla\phi(x)) = T(x)$ when $\phi$ is the true lensing potential,   for any nonzero frequecy vector $l\equiv (l_1, l_2) \in \Bbb R^2$ 
 \[\frac{\partial}{\partial \phi_l}\log P(\phi | \widetilde T,  \text{\rm data}) \propto -  \frac{\phi_l}{C^{\phi\phi}_{l}} -  \sum_{q=1,2} i  l_q \int_{\Bbb R^2} e^{-i x\cdot l} A^q(x)B(x) \frac{dx}{2\pi}   \]
 where  $\phi_l = \re \phi_l + i \im \phi_l$, $\frac{\partial}{\partial \phi_l}\equiv \frac{\partial}{\partial\re \phi_l} + i \frac{\partial}{\partial\im\phi_l}$ and 
 \begin{align}
 B_l &\equiv \frac{1}{C_l^{TT}} \int e^{-i x\cdot l}  \widetilde T(x-\nabla \phi(x))\frac{dx}{2\pi} \\ 
 A^q(x) &\equiv \frac{\partial\widetilde T}{\partial x_q}\bigl(x-\nabla \phi(x)\bigr).
 \end{align}
\end{claim}

The main advantage of this claim is that the gradient can be computed by iterating Fourier transofrms.

Hamiltonian sampler algorithm...

{\rm Remark:}
We also remark that the gradient is also an un-normalized quadratic destimate when the noise is zero and ...
% ----------
\section{Iterative message passing algorithm for $ P(\widetilde T |  \phi,\text{\rm data})$}


\section{Simulation examples}

\section{Concluding remarks}

What we have accomplished

What needs to be done.
\begin{itemize}
 \item High resolution embedding does not scale to Planck (the native resolution is $\sim$10 million pixels)
\item Approximate Gaussian conditional sampling on Planck data resolution (using previous $\phi^{i}$ to seed a conj gradient?)
\item Need fast anti-lensing operations in frequency space to compute $A^q(x)$ and $B(x)$ that does not requires high res $\tilde T$
\item Incorporate spectral density uncertainty
\item Incorporate polarization
\end{itemize}

%%%%%%%%%%%%%%%%%%%%
%
%  appendix
%
%%%%%%%%%%%%%%%%%%%%%%
\appendix

\section{}
%\section{Proofs}

Before we proceed to the proofs we say a few words regarding notation.
Firstly, we do not differentiate, notationally, from the case of smooth random field with periodic boundary conditions defined on $(-L/2, L/2]^2$ and the case where $L\rightarrow \infty$ so that the Fourier series $\sum_{l \in \frac{2\pi}{L}\Bbb Z }   e^{i x\cdot l}  f_l \frac{2\pi/ L}{2\pi} $ converges to the continuous Fourier transform $\int_{\Bbb R^2}  e^{i x\cdot l}  f_l \frac{dl}{2\pi} $. %Indeed,  $\int_{\Bbb R^2}  e^{i x\cdot l}  f_l \frac{dl}{2\pi} $ is simply equivalent notation for the Riemann sum, with $L$ near infinity.
For example, at times we will refer to an infinitesimal area element $dl$ or $dk$ in Fourier space, which simply equals $\frac{2\pi}{L}$ for large $L$. In this case $\delta_l$ denotes a discrete dirac delta function which we equate with $1/dk$ when $l=0$ and zero otherwise. 
Secondly, for any function $f(x)$ let $f^\phi(x) = f(x-\nabla \phi(x))$ denote anti-lensing of $f$ and $f^\phi_l$ denote the Fourier transform of  $f^\phi(x)$.





\begin{proof}[{ Proof of Claim \ref{grad claim}}]
Since $\tilde T$ is sufficient for the unknown $\phi$ we have that 
\begin{align*}
P(\phi|\widetilde T, \text{\rm data}) &= P(\phi|\widetilde T)\propto P(\widetilde T|\phi)P(\phi).
\end{align*}
Since $\phi(x)$ is an isotropic random field with spectral density $C_l^{\phi\phi}$ we have that $E(\phi^{\phantom{*}}_l \phi_{l^\prime}^*) = \delta_{l - l^\prime}C_l^{\phi\phi}$. 
Therefore  $E(\phi_l^{\phantom{*}}\phi_l^*) = \delta_{0}C_l^{\phi\phi}$ and $E(\phi_l\phi_{l}) = 0$ one gets that the random variables $\re \phi_l$, $\im \phi_l$ are independent $\mathcal N(0, \frac{1}{2}\delta_0 C_l^{\phi\phi})$ for each fixed $l$.
Moreovoer  $\phi(x)$ takes values in $\Bbb R$ so that  $\phi_l = \phi_{-l}^*$.  This implies (\textcolor{red}{what exactly implies what here? clearly the moments don't tell us exactly that they are independent since $E(z w^*)=0$ could happen when $w = z^*$, and $(\re z, \im z)^t\sim \mathcal N(0,\sigma^2 I)$. But I think this is the only case where $E(zw^*)=0$ when we know $z$ and $w$ are marginally Gaussian.}) that $\phi_l$ and are independent random variables over all $l$ which are restricted to the a Hermitian half of the Fourier grid, denoted $\Bbb H$. In particular,  if we exclude the zero frequency $l = 0$ we get
\begin{align}
\log P(\phi) - c_1 &=  - \frac{1}{2}\sum_{k \in \Bbb H\setminus \{0 \}}  \left[\frac{(\re \phi_k)^2}{\frac{1}{2}\delta_0 C_k^{\phi\phi}} +  \frac{(\im \phi_k)^2}{\frac{1}{2}\delta_0 C_k^{\phi\phi}} \right] = - \frac{1}{2}\int_{\Bbb R^2} \frac{|\phi_k|^2}{C_k^{\phi\phi}} dk \label{dr1} \\
\log P(\widetilde T| \phi) - c_2 &=  - \frac{1}{2}\sum_{k \in \Bbb H\setminus \{0 \}}  \left[\frac{(\re \widetilde T_k^\phi)^2}{\frac{1}{2}\delta_0 C_k^{TT}} +  \frac{(\im \widetilde T_k^\phi)^2}{\frac{1}{2}\delta_0 C_k^{TT}} \right] = - \frac{1}{2}\int_{\Bbb R^2} \frac{\bigl|\widetilde T_k^\phi\bigr|^2}{C_k^{TT}} dk \label{dr2}
\end{align}
where $c_1$ and $c_2$ are constants and  $\widetilde T^\phi(x)\equiv \widetilde T(x-\nabla \phi(x))$.
% -------- extra detail
% The integral approximation then becomes
% \[ \log P(\phi) = \text{constant} - \pi \int \left[\frac{(\re \phi_l)^2}{C_l^{\phi\phi}} +  \frac{(\im \phi_l)^2}{ C_l^{\phi\phi}} \right] \frac{dl}{2\pi} \]
% -------- extra detail
% Therefore
% \begin{align*}
% \frac{\partial}{\partial \re \phi_l}\log P(\phi)  &= -2 \frac{\re \phi_l}{\delta_0 C_l^{\phi\phi}} \\
% \frac{\partial}{\partial \im \phi_l}\log P(\phi)  &= -2 \frac{\im \phi_l}{\delta_0 C_l^{\phi\phi}}
% \end{align*}
Taking derivatives in (\ref{dr1}) gives
\begin{equation}
\label{grad of prior}
\frac{\partial}{\partial \phi_l}\log P(\phi) = - 2(dl) \frac{\phi_l}{C^{\phi\phi}_{l}}.
\end{equation}
Taking derivatives in (\ref{dr2}) gives
\begin{align}
\frac{\partial}{\partial \re\phi_l}\log P(\widetilde T| \phi) 
% <----- extra detail
%&=  - dk\sum_{k \in \Bbb H} 2 \re \Bigl( \frac{\partial \widetilde T_k^\phi}{\partial \re\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \Bigr)\\
&= - \re \int_{\Bbb R^2} \frac{\partial \widetilde T_k^{\phi}}{\partial \re\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk \label{ll1}\\
\frac{\partial}{\partial \im\phi_l}\log P(\widetilde T| \phi) 
% <----- extra detail
%&=  - dk\sum_{k \in \Bbb H}2 \re \Bigl( \frac{\partial \widetilde T_k^\phi}{\partial \im\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \Bigr) \\
&=  -\re \int_{\Bbb R^2}\frac{\partial \widetilde T_k^{\phi}}{\partial \im\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \, dk \label{ll2}.
\end{align}
Taking linear combinations of the two equalities in Lemma \ref{partialconj} we get
\begin{align}
\frac{\partial \widetilde T^\phi_k}{\partial \re \phi_{l}}  &= 
\frac{1}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi_{l}} + \frac{1}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi^*_{l}}
=
\frac{dk}{2 \pi}  \sum_{q=1,2} il_q\left\{ [(\nabla^q\widetilde T)^\phi]_{k-l}  -   [(\nabla^q\widetilde T)^\phi]_{k+l}  \right\}\\
\frac{\partial \widetilde T^\phi_k}{\partial \im \phi_{l} }  &= 
\frac{-i}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi_{l}} + \frac{i}{2}\frac{\partial \widetilde T^\phi_k}{\partial\phi^*_{l}}
=
\frac{ dk}{2 \pi} \sum_{q=1,2} l_q\left\{ -[(\nabla^q\widetilde T)^\phi]_{k-l}  -   [(\nabla^q\widetilde T)^\phi]_{k+l}  \right\}.
\end{align}
Now the above two equations establish, by Lemma \ref{forreal}, that  both integrals $\int_{\Bbb R^2} \frac{\partial \widetilde T_k^\phi}{\partial \re\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk$ and $\int_{\Bbb R^2}\frac{\partial \widetilde T_k^\phi}{\partial \im\phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}} \, dk$ are real
which implies
\begin{align*}
 \frac{\partial}{\partial \phi_l}\log P(\widetilde T| \phi) &= -\int_{\Bbb R^2} \frac{\partial \widetilde T_k^\phi}{\partial \phi_l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk  \\
 &= -\frac{dk}{ \pi} \sum_{q=1,2} il_q \int_{\Bbb R^2}  [(\nabla^q\widetilde T)^\phi]_{k+l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,dk \\
 &= -  i 2 (dk) \sum_{q=1,2} l_q \int_{\Bbb R^2}  [(\nabla^q\widetilde T)^\phi]_{k+l} \, \frac{\widetilde T_k^{\phi^*}}{C_k^{TT}}  \,\frac{dk}{2\pi} \\
 & = -  i 2 (dk) \sum_{q=1,2} l_q \int_{\Bbb R^2} e^{-i x\cdot l} A^q(x) \, B(x)  \,\frac{dx}{2\pi},\quad\text{by Lemma \ref{conv}}
 \end{align*}
 where $A^q(x) \equiv (\nabla^q\widetilde T)^\phi(x)$ and $B_k\equiv (\widetilde T_k^{\phi})^* / C_k^{TT}$.
\end{proof}
% end proof



%%%%%%%%%%%%%%% lemma
\begin{lemma} 
\label{partialconj}
\begin{align}
\frac{\partial \widetilde T^\phi_k}{\partial \phi_{ l} }  &= \frac{  dk}{\pi}\sum_{q=1,2} -il_q[(\nabla^q\widetilde T)^\phi]_{k+l} \\
\frac{\partial \widetilde T^\phi_k}{\partial \phi^*_{ l} }  & = \frac{ dk }{\pi}\sum_{q=1,2} \phantom{-}\!\!il_q [(\nabla^q\widetilde T)^\phi]_{k-l}
\end{align}
where $\nabla^q \widetilde T\equiv \frac{\partial \widetilde T}{\partial x_q}$.
\end{lemma}
\begin{proof}
First notice
\begin{align}
\label{partial1}
\frac{\partial}{\partial \re \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q}  &= \int_{\Bbb R^2} i  k_q e^{ix \cdot k} \frac{\partial \phi_k}{\partial \re \phi_l }  \frac{d k}{2\pi} 
=\left[i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  \right] \frac{d k}{2\pi}   \\
\label{partial2}
\frac{\partial}{\partial \im \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q}  &= \int_{\Bbb R^2} i  k_q e^{ix \cdot  k} \frac{\partial \phi_ k}{\partial \im \phi_{l} }  \frac{d k}{2\pi} 
=\left[-  l_q e^{ix \cdot  l}  -  l_q e^{-i x \cdot  l}  \right] \frac{d k}{2\pi}.  
\end{align}
This implies
\begin{align}
\nonumber \frac{\partial \widetilde T^\phi_k}{\partial \phi_{ l} } 
&=  \frac{\partial}{\partial  \phi_{ l} } \int_{\Bbb R^2}  e^{-ix \cdot k}\widetilde T(x+\nabla \phi(x))\frac{dx}{2\pi} \\
\nonumber &= \sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ \frac{\partial}{\partial \re \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q} +i  \frac{\partial}{\partial \im \phi_{ l}}\frac{\partial\phi(x)}{ \partial x_q}\right]\frac{dx}{2\pi}\\
% <-----extra detail
%  &= \frac{d k}{2\pi} \sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  + i(-  l_q e^{ix \cdot  l}  -  l_q e^{-i x \cdot  l})  \right]\frac{dx}{2\pi} \\
% &= \frac{d k}{2\pi}  \sum_{q=1,2} i l_q \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ e^{ix \cdot  l}  -  e^{-i x \cdot  l}  -  e^{ix \cdot  l}  -  e^{-i x \cdot  l} \right]\frac{dx}{2\pi}\\
% <-----
\nonumber &= \sum_{q=1,2}\frac{ - il_q  dk}{\pi}  \int_{\Bbb R^2} e^{- ix \cdot (k+l)} \nabla^q\widetilde T(x+\nabla \phi(x))\frac{dx}{2\pi},\quad\text{by (\ref{partial1}) and (\ref{partial2})} \\
&=\sum_{q=1,2}\frac{ - il_q dk}{\pi} [(\nabla^q\widetilde T)^\phi]_{k+l}
\end{align}
Similarly 
\begin{align}
\frac{\partial \widetilde T^\phi_k }{\partial \phi^*_l } 
% <-----extra detail
% &= \frac{d k}{2\pi}\sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  - i(-  l_q e^{ix \cdot  l}  -  l_q e^{-i x \cdot  l})  \right]\frac{dx}{2\pi} \\
% &= \frac{d k}{2\pi} \sum_{q=1,2}  \int_{\Bbb R^2} e^{-ix \cdot k} \nabla^q\widetilde T(x+\nabla \phi(x))\left[ i  l_q e^{ix \cdot  l}  - i  l_q e^{-i x \cdot  l}  + i  l_q e^{ix \cdot  l}  +i  l_q e^{-i x \cdot  l}  \right]\frac{dx}{\pi} \\
&= \sum_{q=1,2}  \frac{i l_qd k}{\pi}\int_{\Bbb R^2} e^{-ix \cdot (k-l)} \nabla^q\widetilde T(x+\nabla \phi(x))\frac{dx}{\pi} \\
% <-------
 &=\sum_{q=1,2}\frac{ il_q dk}{\pi} [(\nabla^q\widetilde T)^\phi]_{k-l}.
\end{align}
\end{proof}





%%%%%%%%%%%% lemma
\begin{lemma} 
\label{forreal}
If $A(x)$ and $B(x)$ are real scalar fields then  the two  integrals,  $\int_{\Bbb R^2} i\bigl\{ A_{k-l}  -   A_{k+l}  \bigr\} B^*_k dk$ and  $\int_{\Bbb R^2}\bigl\{  A_{k-l}  +    A_{k+l}   \bigr\} B^*_k dk$, are both real numbers.
\end{lemma}


\begin{proof}
By a simple change of variables it is clear that 
$\int_{\Bbb R^2} \left(i\bigl\{ A_{k-l}  -   A_{k+l}  \bigr\} B^*_k\right)^* dk = \int_{\Bbb R^2} i\bigl\{ A_{k^\prime-l} - A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime$ and $\int_{\Bbb R^2} \left(\bigl\{  A_{k-l}  +    A_{k+l}   \bigr\} B^*_k \right)^* dk  = \int_{\Bbb R^2} \bigl\{ A_{k^\prime-l} + A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime$.

% \begin{align*}
% \int_{\Bbb R^2} \left(i\bigl\{ A_{k-l}  -   A_{k+l}  \bigr\} B^*_k\right)^* dk &= \int_{\Bbb R^2} -i\bigl\{ A_{-k+l}  -   A_{-k-l}  \bigr\} B_{k} dk \\
% &= \int_{\Bbb R^2} -i\bigl\{ A_{k^\prime+l}  -   A_{k^\prime-l}  \bigr\} B_{-k^\prime} dk^\prime \\
% &= \int_{\Bbb R^2} i\bigl\{ A_{k^\prime-l} - A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime
% \end{align*}

% \begin{align*}
% \int_{\Bbb R^2} \left(\bigl\{  A_{k-l}  +    A_{k+l}   \bigr\} B^*_k \right)^* dk &= \int_{\Bbb R^2} \bigl\{ A_{-k+l}  +   A_{-k-l}  \bigr\} B_{k} dk \\
% &= \int_{\Bbb R^2} \bigl\{ A_{k^\prime+l}  +   A_{k^\prime-l}  \bigr\} B_{-k^\prime} dk^\prime \\
% &= \int_{\Bbb R^2} \bigl\{ A_{k^\prime-l} + A_{k^\prime+l}  \bigr\} B_{k^\prime}^* dk^\prime
% \end{align*}
\end{proof}



%%%%%%%%%%%% lemma
\begin{lemma} 
\label{conv}
If $A(x)$ and $B(x)$ are real scalar fields then  $\int_{\Bbb R^2} A_{k+l}  B^*_k \frac{dk}{2\pi}= \int_{\Bbb R^2} e^{-ix\cdot l} A(x)B(x)\frac{dx}{2\pi}$.
\end{lemma}

\begin{proof} 
\begin{align*}
\int_{\Bbb R^2} A_{k+l}  B^*_k \frac{dk}{2\pi} 
& = \int_{\Bbb R^2} \int_{\Bbb R^2} e^{-x\cdot (k+l)} A(x) B_{k}^*  \frac{dx}{2\pi}   \frac{dk}{2\pi} \\
& = \int_{\Bbb R^2}e^{-ix\cdot l} A(x) \left[\int_{\Bbb R^2} e^{-x\cdot k} B_{k}^*     \frac{dk}{2\pi}\right] \frac{dx}{2\pi}\\
& = \int_{\Bbb R^2}e^{-ix\cdot l} A(x) \left[\int_{\Bbb R^2} e^{x\cdot k} B_{k}     \frac{dk}{2\pi}\right]^* \frac{dx}{2\pi}\\
&= \int_{\Bbb R^2} e^{-ix\cdot l} A(x)B^*(x)\frac{dx}{2\pi}\\
&= \int_{\Bbb R^2} e^{-ix\cdot l} A(x)B(x)\frac{dx}{2\pi},\quad\text{since $B(x)$ is real.}
\end{align*}

\end{proof}




\end{document}

